# 회귀 알고리즘과 모델 규제(3-2, 3-3) - 3주차

**3-3**까지 내용을 정리함을 서두에 밝힙니다.<br>
책 내용에서 보다 깊이 있게 다루면 좋을 요소 위주의 정리를 진행했습니다. 따라서 책 전체 내용의 기술되어 있지 않을 수 있습니다. 궁금한 사항이나, 잘못된 부분이 있다면, 언제든 issue에 의견을 남겨주시면 감사하겠습니다. 해당 내용은 **데이터 마이닝  - 이상민 교수님**자료를 참고하여 만들었습니다. 

## 선형회귀 분류체계(Taxonomy)
기본적으로 regression은 아래와 같은 분류 체계를 가집니다.
![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/48356954/bcb5545b-4968-4ad2-8b48-936eb9a80826)
<br>

## 정규성 가정
사실 선형회귀를 진행하기 앞서 가장 중요한 것이 바로 **정규성 가정**입니다. 책에서 학습한 모든 선형회귀 내용은 해당 가정사항을 만족한다는 전재로 만들어 졌습니다. 그렇기 때문에, 정규성 가정이 무엇인지 알아보겠습니다.
<br><br>

![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/48356954/8e956714-c095-43af-8f2c-b7abd1099418)
<br>
해당 그림에서 선형회귀 모델로 설명할 수 없는 error값을 입실론으로 정의합니다. 그리고 해당 입실론이 정규분포라고 가정합니다. 이것이 바로 정규성 가정이자, 이를 기반으로 선형회귀 모델에 적용할 수 있습니다. 해당 개념을 수식으로 나타내면 다음과 같습니다. <br>

![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/48356954/4fe8904e-843c-4e56-8645-acd49fa859cb)

### 선형회귀 적용
방금 언급한 정규성 가정을 선형회귀 모델에 적용하면 어떨까요? 아래 수식을 보면서 이해해 보겠습니다. <br>
![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/48356954/0f9d61de-079c-45cd-8db4-6987f053758e)<br>
해당 수식에서 Y는 선형회귀 모델입니다. 이 모델의 평균과 분산을 구하려면, 앞서 정의한 입실론의 정규성 가정 수식이 필요합니다. 입실론은 평균 0, 분산 (시그마)^2 의 정규분포를 가지기 때문에, 이를 적용하면 위와 같은 결과가 도출됩니다. 즉, 선형회귀 모델 역시 정규분포를 가지게 됩니다. 이를 도식화 해보면 아래 그림과 같이 나타낼 수 있습니다. <br><br>
![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/48356954/35b8b975-7803-478a-93b2-35f57de170c7)

### 결론
최종적으로 우리가 구하고자 하는 선형회귀 모델이 가장 잘 적합될 수 있는 파라미터 베타를 찾는것이 선형회귀의 핵심입니다. 조금 더 명확하게 정의하면 입력변수(x)와 출력변수(Y) 평균과의 관계를 설명하는 식을 찾고, 이를 가장 잘 설명하는 파라미터를 찾는 것이 선형회귀의 핵심입니다. 
![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/48356954/7676c9a8-98e9-4f39-8f9b-fa5ef8c53d47)
<br>
## Regularization(정규화)
정규화는 기본적으로 train과정에서의 overfitting을 방지하기 위해 사용하는 기법 중 하나입니다. 선형회귀 모델의 특성상 베타값의 크기가 출력값에 많은 영향을 줍니다. 가령, 베타값이 굉장히 큰 값(y = 10000000x + e)이라면 x값이 변할때마다 y값 역시 굉장히 민감하게 반응합니다. 그렇기 때문에 베타값을 낮추면서 좋은 성능을 보여주는 모델을 만드는 것이 중요합니다. 이를 overfitting과 연관지으면, 베타가 크다는 것은 train data에 적합하게 학습이 된 것이고, 일반화 하기 어렵다고 해석할 수 있습니다. 따라서 베타의 크기를 학습하는 것이 **정규화** 라고 볼 수 있습니다.
![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/48356954/db133829-cb22-4081-a6d0-edfed9cc270f)
![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/48356954/77bd6ce7-627d-441b-ac75-a5ab0e307baf)
<br>
위 수식에서 나온 파이값이 바로 베타의 크기를 결정하는 요소입니다. 즉, 우리의 목적은 해당 loss를 최소화 하는 베타값을 찾는것이 목적입니다. 이를 아래와 같이 표현할 수 있습니다.<br>
![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/48356954/35abc4b0-b7e9-43be-aa78-741e37271603)
<br>
즉, **i)는 베타의 크기를 학습하는 과정이고, ii)는 베타가 커지는 것을 방지하는 것입니다.**
여기서의 람다값이 조절하는 알파값으로 해석할 수 있습니다. 
