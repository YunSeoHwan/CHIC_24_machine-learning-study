### 용어
- 다중공정성 : 변수간의 상관관계
- 회귀 : 두 변수 사이의 상관관계를 분석하는 방법(독립변수를 가지고 종속변수 찾기)
- 모델 파라미터 : 선형 회귀가 찾은 가중치처럼 머신러닝 모델이 특성에서 학습한 파라미터
- 하이퍼 파라미터 : 사용자가 지정한 값
- 다중 회귀 : 여러 개의 특성을 사용하는 회귀 모델. 특성이 많으면 선형 모델은 강력한 성능을 발휘한다.
- 릿지 : 규제가 있는 선형 회귀 모델 중 하나이며 선형 모델의 계수를 작게 만들어 과대적합을 완화시킨다.
- 라쏘 : 규제가 있는 선형 회귀 모델 중 하나이며 릿지와 달리 계수 값을 아예 0으로 만들 수 있다.
- 하이퍼파라미터 : 머신러닝 알고리즘이 학습하지 않는 파라미터이며 사람이 사전에 지정한다. 대표적으로 릿지와 라쏘의 규제 강도 alpha 파라미터이다.

# 3-1. k-최근접 이웃 회귀  

- 사이킷런에 사용할 훈련 세트는 2차원 배열이어야 한다.  

- 넘파이는 배열의 크기를 자동으로 지정하는 기능 제공(크기에 -1 지정시, 나머지 원소 개수로 모두 채우라는 의미)  
  ex) 첫 번째 크기를 나머지 원소 개수로 채우고, 두 번째 크기를 1로 하려면 train_input.reshape(-1, 1) 사용

## 결정계수(R^2)
- R^2 = 1 - (타깃 - 예측)^2 의 합 / (타깃 - 평균)^2 의 합
- 1 - SSE/SST = SSR
- SST = SSR(설명이 가능) + SSE(설명X)
- 즉, SSR/SSR = R^2 의 값이 1에 가까울 수록 이 모델이 모두 설명할 수 있다고 해석된다.
- 평균 정도를 예측하는 수준이라면, R^2은 0에 가까워지고, 예측이 타깃에 가까워지면 1에 가까워진다.
  
#### score() 메서드의 출력값의 의미  
: score() 메서드가 에러율을 반환한다면 이를 음수를 만들어  
실제로는 낮은 에러가 score()메서드로 반환될 때는 높은 값이 되도록 바꿉니다.

#### bias(편향) 와 variance(분산)중 어떤것이 높을수록 안 좋을까?
: variance가 높을수록 더 안 좋다  
모델의 복잡성 때문에 -> 해결법: 가벼운 모델을 사용한다. 

#### 과소적합이 일어나는 이유
훈련 세트와 테스트 세트의 크기가 매우 작기 때문에

### R^2 과 adj R^2 에 대한 이야기
- R^2 는 특성계수가 많을수록 높을 수 밖에 없다
- adj R^2 는 특성 계수를 고려한다. 


# 3-2. 선형 회귀
- 시간과 환경이 변화하면서 데이터도 바뀌기 때문에 새로운 데이터를 사용해 반복적으로 훈련해야 한다.

## 선형 회귀  
: 직선의 위치가 훈련 세트의 평균에 가깝다면 R^2는 0에 가까운 값이 된다.  
but, 예측을 반대로 하면 R^2은 음수가 될 수 있다. 

<img width="80%" src="https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/73346564/0e444b61-72f8-4784-a987-70a7de402400"/>

- LinearRegression 클래스가 찾은 기울기 a 와 y절편 b는 lr 객체의 coef_ 와 intercept_ 속성에 저장되어 있다.
- coef_ 속성 이름에서 알 수 있듯이 머신러닝에서 기울기를 종종 계수(coefficient) 또는 가중치(weigt)라고 부른다.

## 다항 회귀
: 다항식을 사용하여 특성과 타깃 사이의 관계를 나타낸다. 이 함수는 비선형일 수 있지만 선형 회귀로 표현할 수 있다. 
- 2차 방정식의 그래프를 그리려면 제곱한 항이 훈련 세트에 추가 되어야 한다.  
ex. 원래 특성인 길이를 제곱하여 추가했기 때문에 훈련 세트와 테스트 세트 모두 열이 2개로 늘어났다.  
주목할 점은, 2차 방정식 그래프를 찾기 위해 훈련 세트에 제곱 항을 추가했지만, 타깃값은 그대로 사용한다는 것이다. 목표하는 값은 어떤 그래프를 훈련하든지 바꿀 필요가 없다. 단, 제곱한 값과 원래 길이를 함께 넣어주어야 한다. 

> #### 2차 방정식도 선형 회귀라고 하나요?
> 제곱한 길이를 간단히 다른 변수로 치환하여서 진행하면 선형 관계로 표현 할 수 있다.

# 3-3 특성 공학과 규제  
- 특성이 많은 고차원에서는 선형 회귀가 매우 복잡한 모델이 될 수 있다.
> #### include_bias = False 는 꼭 지정해야 하는지?
> include_bias = False로 지정하지 않아도 사이킷런 모델은 자동으로 특성에 추가된 절편 항을 무시

#### 릿지(Ridge)
: 릿지는 계수를 제곱한 값을 기준으로 규제를 적용

#### 라쏘(Lasso)
: 라쏘는 계수의 절댓값을 기준으로 규제를 적용

> 일반적으로, 릿지를 더 선호한다.  
> 두 알고리즘 모두 계수의 크기를 줄이지만, 라쏘의 경우 아예 0으로 만들 수 있기 떄문이다.  
> 이러한 라쏘의 특징 때문에 라쏘 모델을 유용한 특성을 골라내는 용도로도 사용할 수 있다.


# 질문

#### Q.길이를 제곱하면 선형적인가?
A. 선형적이다-> 직선형태로 선형적으로 표현.
각각의 변수를 더하기 및 빼기로 표현, 모든 식을 선형결합으로 표현. 선형적이어야 한다.

제곱하는 것-> 하나의 변수로 대체를 하면 선형 결합인 것이다. 
루트, 로그 -> 대체하는 순간 값이 꼬이게 된다. 

샘플 개수보다 특성이 많다는 것 : 샘플 : row , 특성 : column(열)
-> 데이터의 개수보다 특성이 많다
행의 개수보다 열의 개수가 많다(가로가 엄청 긴 직사각형)

##### -  샘플 개수가 특성보다 많아야한다. 그러지 않으면 왜곡된 정보가 나올 수 있기 때문에


#### Q. 제곱하면 0 아래로 안 내려가는지
A. 내려갈 수는 있지만 이 상황에서는 안 내려가는게 최선의 방식이다.

#### Q. 특성에 대한 부분에 대한 질문
A. 타깃-> 종속변수 / 특성 -> 독립변수 

##### - 선형회귀란, 여러개의 독립변수를 이용해서 하나의 종속변수를 맞추는 것


#### Q. numpy 와 pandas 차이
A. numpy는 array 행렬연산. pandas는 dataframe 

#### Q. 제곱하면 왜곡되지 않는지
A. 그렇기에 scaling을 사용한다. 
