### <두 번째 스터디 – 2024.01.19> 
### 범위: p.87~p.129
- 질의응답 중심으로 스터디를 진행하였습니다.
### Chapter 3 회귀 알고리즘과 모델 규제
##### 1. 회귀 알고리즘?(p.115)
- 회귀는 두 변수(독립변수)사이의 상관관계를 분석하는 방법 -> 종속변수를 찾는 과정
- 변수간의 상관관계를 파악하기 위해 사용 
- 선형회귀는 변수간 독립성을 가정하고 사용하는 것. 독립성이 보장되지 않으면 당연히 선형회귀 결과는 잘 나올 것
※ 다중공산성 PI CI VIF
##### 편향(bias)과 분산(variance)의 관계?
- trade off 하다:  (반비례는 아니지만..)
- 모델 예측 오차 vs 모델 예측 편차 
- low variance: 한 점에 몰려 있다
- bias: 영점 조절
- variance가 높으면 실력이 사격 실력이 안좋은 것. -> 딥러닝 : 할 때 마다 결과가 달라진다. -> variance가 높을 수 밖에 없다 (모델의 복잡성)
- bias가 높더라도 variance를 낮추려고 노력해야 한다.(물론 둘 다 낮으면 좋음)
- overfitting : variance가 높고 bias가 낮은 것. "일반화되지 못함"
- 딥러닝 같은 경우에 variance를 줄이기 위한 조치: parameter 줄이기, 가벼운 모델 쓰기 등등
### <세 번째 스터디 – 2024.01.28> 
### 범위: p.130~p.173
##### p.138에서는 일부 데이터의 예측값이 0 아래인데, 선형회귀에서도 직선이 0 이상이 되어야하는 것 아닌가?
- 알고리즘을 들여다보면,  실제 데이터와 가장 작은 차이를 가지게 하는 직선을 학습한 결과로 나온 직선임. 
##### p.139에서 길이의 제곱을 통해 다항회귀를 구현하였는데 로그나 루트 등 다른 방법을 사용할 수 있는지?
- 로그나 루트를 통한 변환을 불가능하다. 선형회귀 : 직선/평면 형태이므로 선형결합으로 표현이 가능해야 한다. 선형결합: 각각의 변수를 더하기/빼기를 통해 표현
- 제곱이나 세제곱 등은 치환이 되지만  로그나 루트를 씌우면 선형이 아니다. 로그,루트는 x<0에서 mapping 불가능
##### P.142에서 제곱을 했다고 해서하면 그래프가 0 이하로 내려가지 않는 것이 보장되는 것인가?
- 내려갈 수 있는데, 우리 데이터(샘플)에서는 양의 값을 가지는 값들끼리의 선형결합이므로.. 수식을 이차함수라고 가정하면 최솟값을 가지는 경우가 있을 것.
- 특성이 늘어나면 더욱 복잡한 형태가 될 것임. 그러면 2차원에서 표현하기 어렵다. 충분히 내려가는 것이 가능하다.
책에서의 데이터셋으로는 내려갈 수 없다.
※ scaler도 하나의 학습이다. scaler를 통해 0~1사이의 값으로 바꾸기 때문에 다시 원본 상태로 변환할 필요가 있다(transform으로 변환할 수 있다.)
##### P.151 하나의 특성을 사용한 것의 의미?
- 타깃: 종속변수
- 특성: 독립변수 ★ 다른 것에 영향을 받지 않아야한다. "독립변수들이 서로 독립적이여야 한다"
- 선형회귀는 여러 개의 독립변수를 통해서 하나 이상의 종속변수를 찾아내는것이 선형회귀임
- y=ax에서, y는 종속변수이고, x는 독립변수(특성)이다.
##### pandas와 numpy간의 관계
- pandas는 dataframe 
- numpy는 array -> 행렬을 나타낼 때 사용하는 것

※ polynominal: 다항식. 선형결합을 의미함

##### p.158의 샘플의 개수가 특성의 개수보다 많았을 때 발생할 수 있는 문제
- ->샘플개수가 많아야 한다. 예를 들어, 특성 10개인데 샘플이 3개이면 왜곡된 정보. 특성들을 설명할 수 없다
- 예시:  병원에서 신체정보를 통해 암을 예측하려 한다. 신체정보가 20개인데, 사람이 3명이면 설명하기에 부족하다(정확하지 않을 확률이 높다)
- colum수가 행의 수가 더 많은 경우이다. -> 차원의 저주
##### 선형회귀
- 선형회귀에서 필요한 것: 정규성 가정
- 정규성 가정: 선형회귀 모델로 설명할 수 없는 error값(입실론)이 정규분포를 따른다고 가정 
- 수식적인 basis를 통해 선형회귀 모델 또한 정규분포를 따르게 되는 것을 확인할 수 있음.
- 선형회귀의 목적: loss를 최소화 하는 베타값 찾기
- MSE(학습 오류) + 일반화 성능(λΣ) 에서의 람다값: 릿지 라쏘 회귀에서의 ⍺값.