# 회귀 
: 임의의 어떤 숫자를 예측하는 문제, 두 변수 사이의 상관관계를 분석하는 방법

- 지도 학습 알고리즘은 크게 분류와 회귀로 나눔

# K-최근접 이웃 회귀 모델
- k-최근접 이웃 알고리즘을 사용해 회귀 문제를 품
- 가까운 k개의 이웃을 찾음 → 이웃 샘플의 타킷값을 평균해 이 샘플의 예측값으로 사용
- 하나의 특성 사용시
  - 특성 데이터를 x축에 놓고 타킷 데이터를 y축에 놓음
  - 사이킷런의 train_test_split() 함수를 사용해 훈련 세트와 테스트 세트 나눔
  - 사이킷런 사용시 훈련 세트는 2차원 배열이어야 함 → 넘파이 배열의 크기를 바꿀 수 있는 reshape() 메서드 사용
    ```
    ex) test_array = test_array.reshape(2,2)
    ```

# 결정계수(R^2)
- KNeighborsRegressor : 사이킷런에서 k-최근접 이웃 회귀 알고리즘을 구현한 클래스, KNeighborsClassifier와 사용 비슷, 객체 생성하고 fit() 메서드로
                        회귀 모델 훈련
- 회귀에서는 정확한 숫자를 맞힌다는 것은 거의 불가능 (→ 예측하는 값이나 타깃 모두 임의의 수치이기 때문)
- 결정계수(R^2) : 
  - 계산 : R^2 = 1 - (타깃 - 예측)^2의 합 / (타킷 - 평균)^2의 합
  - 타깃의 평균 정도를 예측하는 수준이라면 결과 값 0에 가까워짐(즉 분자와 분모가 비슷해짐)
  ` 예측이 타깃에 아주 가까워지면 1에 가까운 값이 됨(분자가 0에 가까워짐)
- 사이킷런의 score() 메서드 : 출력값이 높을수록 좋음(정확도나 결정계수의 경우), 
                              에러율 반환의 경우 이를 음수로 만들어 실제로는 낮은 에러가 score() 메서드로 반환될 떄는 높은 값이 되도록 바꿈
- 타깃과 예측한 값 사이의 차이를 구해보면 어느 정도 예측이 벗어났는지 가늠하기 좋음
- mean_absolute_error : sklearn.metrics 패키지 아래 여러 측정 도구 중 하나, 타깃과 에측의 절댓값 오차를 평균해 반환

# 과대적합, 과소적합
- 보통 훈련 세트의 점수가 조금 더 높게 나옴.( 훈련 세트에서 모델을 훈련했으므로 훈련 세트에서 더 좋은 점수가 나옴)
- 훈련 세트의 점수와 테스트 ㅔㅅ트의 점수 차이가 크면 좋지 않음
- 과대적합 : 테스트 세트의 점수가 너무 낮다면 모델이 훈련 세트에 과도하게 맞춰진 것, 모델을 덜 복잡하게 만들어야 함 → k-최근접 이웃의 경우 k값 늘림
- 과소적합 : 테스트 세트 점수가 너무 높거나 두 점수가 모두 낮은 경우, 모델을 더 복잡하게 만들어야 함 → k-최근접 이웃의 경우 k값 줄
