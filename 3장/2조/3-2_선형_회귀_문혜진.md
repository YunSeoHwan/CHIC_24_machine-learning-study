# 선형 회귀로 훈련 세트 범위 밖의 샘플 예측
- k-최근접 이웃 회귀를 사용해 예측할 경우 → 훈련 세트 범위 밖의 샘플을 예측할 수 없음
  → 아무리 멀리 떨어져 있더라도 무조건 가장 가까운 샘플의 타깃을 평균하여 예측하기 떄문
- 위의 문제를 해결하기 위해 선형 회귀 사용

# 선형회귀
: 특성과 타깃 사이의 관계를 가장 잘 나타내는 선형 방정식을 찾음 
- 특성이 하나면 직선 방정식이 됨
- 특성이 하나인 경우 어떤 직선을 학습하는 알고리즘 => 그 특성을 가장 잘 나타낼 수 있는 직선 찾아야함.
- 훈련 세트에 잘 맞는 직선의 방정식(= 최적의 기울기와 절편을 구함)을 찾는 것 
- 선형 회귀가 찾은 특성과 타깃 사이의 관계느 ㄴ선형 방정식의 계수 또는 가중치에 저장 됨
- 대표적인 회귀 알고리즘, 비교적 간단하고 성능이 뛰어남
- LinearRegression 클래스 사용
  - 선형회귀, 다항회귀, 다중 회귀 지원
  - 사이킷런 sklearn.linear_model 패키지 아래에 있음
  - k-최근접 이웃 알고리즘을 사용했을 때와 동일한 방식으로 모댈 훈련하고 예측에 사용
  - 선형 회귀 알고리즘 구현
  - 하나의 직선을 그리려면 기울기와 절편 있어야 함 => y = ax + b
    - LinearRegression 클래스가 찾은 a : lr.coef_ , lr객체의 coef_ 속성에 저장, 머신러닝에서 기울기를 종종 계수 또는 가중치로 부름
    - LinearRegression 클래스가 찾은 b : lr.intercept_ , lr객체의 intercept_ 속성에 저장
    ▲ coef_와 intercept_를 머신러닝 알고리즘이 찾은 값이라는 의미로 모델 파라미터라고 부름
      ○ 머신러닝 알고리즘의 훈련 과정은 최적의 모델 파라미터를 찾는 것과 같음 이를 모델 기반 학습이라고 함
      ○ k-최근접 이웃에는 모델 파라미터가 없음, 즉 훈련 세트를 저장하는 것이 훈련의 전부 이를 사례 기반 학습이라고 함

# 다항회귀
: 다항식을 사용해 특성과 타깃 사이의 관계를 나타냄
- 다항식을 사용한 선형 회귀
- 비선형일 수 있지만 여전히 선형 회귀로 표현할 수 있음(교재 141p 여기서 잠깐 부분 참고)
- 예측을 할 때 직성 그래프 -> 무게가 음수가 되는 경우가 생김 -> 현실에서 불가능 -> 최적의 직선 찾기가 아닌 최적의 곡선 찾기!
- 2차 방정식 그래프 : y = a*길이^2 + b*길이 + c
