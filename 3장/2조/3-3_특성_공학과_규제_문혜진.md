# 다중회귀
: 여러 개의 특성을 사용한 선형 회귀
- 특성이 1개인 경우 선형 회귀 모델이 학습하는 것은 직선
- 특성이 2개면 선형 회귀는 평면을 학습
  - 타깃값과 함께 3차원 공간을 형성
  - 선형 회귀 방정식 : 타깃 = a*특성1 + b*특성2 + 절편
- 특성이 많으면 선형 모델은 강력한 성능을 발휘
- 특성이 많은 고차원에서는 선형 회귀가 매우 복잡한 모델 표현 가능
- 특성 공학 : 기존의 주어진 특성을 사용해 조합하여 새로운 특성을 만드는 일련의 작업 과정
  - 직접 추가 가능
  - 사이킷런에서 제공하는 도구 사용하는 방법도 있음

# 사이킷런의 변환기
- 사이킷런은 특성을 만들거나 전처리하기 위해 다양한 클래스 제공 이런 클래스를 변환기라고 함
- 변환기 클래스는 모두 fit(), transform() 메서드 제공
*LinearRegression 같은 사이킷런의 모델 클래스는 추정기라고 부름
- PolynomialFeatures 클래스
  - sklearn.preprocessing 패키지에 포함되어 있음
  - 기본적으로 각 특성을 제곱한 항을 추가하고 특성끼리 곱한 항을 추가함
  - 클래스의 객체를 만든 다음 fit(), transfrom() 메서드 차례로 호출(→ 훈련을 해야 변환이 가능하므로)
    - fit() 메서드 : 새롭게 만들 특성 조합을 찾음
    - transform() 메서드 : 실제로 데이터 변환
    ```
    ex)
      poly = PolynomialFeatures(include_bias=False) 
      # ↑ : 사이킷런의 선형 모델은 자동으로 절편 추가함, 굳이 이렇게 특성 만들 필요 없어 include_bias=False 지정해 특성 변환 (교재 155p참고)
      poly.fit([[2,3]])
      print(poly.transform([[2,3,]]))
    ```
    ▲ 변환기는 입력 데이터를 변환하는데 타깃 데이터 필요X → 모델 클래스와 다르게 fit()메서드에 입력 데이터만 전달
  
  - get_feature_names() 메서드 : 특성들이 각각 어떤 입력의 조합으로 만들어졌는지 알려줌
  - degree 매개변수 : 필요한 고차하으이 최대 차수 지정 가능
    ```
    ex)
      poly = PolynomialFeatures(degree=5, include_bias=False) 
    ```
  ==> 특성의 개수를 크게 늘리면 선형 모델은 아주 강력해짐, 훈련 세트에 대해 거의 완벽하게 학습 가능 but 이렇게되면 훈련 세트에 너무 과대적합되므로
      테스트 세트에서는 형편없는 점수를 만듦 → 선형 회귀 모델을 제약하기 위한 도구가 필요(규제)
      
# 규제
: 머신러닝 모델이 훈련 세트를 너무 과도하게 학습하지 못하도록 훼방하는 것 → 모델이 훈련 세트에 과대적합되지 않도록 만드는 것
- 선형 회귀 모델의 경우 특성에 곱해지는 계수(또는 기울기)의 크기를 작게 만드는 일
- 특성의 스케일이 정규화되지 않으면 곱해지는 계수 값도 차이 
  → 일반적으로 선형 회귀 모델에 규제를 적용하 ㄹ때 계수 값의 크기가 서로 많이 다르면 공정하게 제어도지 않을 것 → 정규화 해야 함!!
- StandardScaler 클래스
  - 표준 점수로 변환해 정규화하는 방법
  - 사이킷런에서 제공하는 변환기 중 하나
  - 훈련 세트로 학습한 변환기를 사용해 테스트 세트까지 변환해야함!
- 선형 회귀 모델에 규체를 추가한 모델을(규제가 있는 선형 회귀 모델) 릿지와 라쏘라 부름 
 - 릿지 : 계수를 제곱한 값을 기준으로 규제 적용
 - 라쏘 : 계수의 절댓값을 기준으로 규제 적용
 - 일반적으로 릿지를 조금 더 선호 
 - 두 알고리즘 모두 계수의 크기를 줄이지만 라쏘는 아예 0으로 만들 수도 있음
 - 사이킷런 위의 두 알고리즘 모두 제공(sklearn.linear_model 패키지 안에 존재)
 
 # 릿지 회귀, 라쏘 회귀
 - 릿지 회귀
   - fit() 메서드에서 훈련한 다음 score() 메서드로 평가
     ```
     ex)
        from sklearn.linear_model import Ridge
        ridge = Ridge()
        ridge.fit(train_scaled, train_target)
        print(ridge.score(train_scaled, train_target)
    ```
  - alpha 매개변수 : 구제의 강도(규제의 양) 조절
    * alpha 값은 사전에 사람이 지정해야하는 값 즉 머신러닝 모델이 학습할 수 없고 사람이 알려줘야하는 파라미터인 하이퍼파라미터임
     (사이킷런과 같은 머신러닝 라이브러리에서 하이퍼파리미터는 클래스와 메서드와 매개변수로 표현)
    - 값↑ : 규제 강도 세짐, 계수 값을 더 줄이고 조금 더 과소적합되도록 유도
    - 값↓ : 계수를 줄이는 역할이 줄어들고 선형 회귀 모델과 유사해지므로 과대적합될 가능성이 큼
    - 적절한 alpha값 찾기 : ahpha 값에 대한 R^2 값의 그래프를 그려보는 것
      - 훈련 세트와 테스트 세트의 점수가 가장 가까운 지점이 최적의 alpha 값
- 라쏘 회귀
  - 릿지와 비슷, Ridge 클래스를 Lasso 클래스로 바꾸는 것이 전부
  - 라쏘는 아예 0으로 만들 수도 있음 → 라쏘 모델의 계수는 coef_속성에 저장되어 있기 때문
    => 라쏘 모델의 경우 0 값이 아닌 특성만을 사용함 ex) 55개 특성 중 40개 계수가 0이면, 실제로 사용한 특성은 15개뿐임
       ~> 이 특징을 활용해 라쏘 모델을 유용한 특성을 골라내는 용도로도 사용 가능!!
