###  k최근접 이웃 회귀와 선형 회귀 알고리즘의 차이를 이해하고 사이킷런을 사용해 여러가지 선형 회귀 모델을 만들기.
<br>

## k-최근접 이웃의 한계
- k-최근접 이웃 회귀는 가장 가까운 샘플을 찾아 타깃을 평균하므로, **새로운 샘플이 훈련 세트의 범위를 벗어나면 엉뚱한 값으로 예측할 수 있다!!!**
- k-최근접 이웃 회귀의 문제를 해결하려면 가장 큰 샘플이 포함되도록 훈련 세트를 다시 만들어야 한다.
  
<br>

## 선형 회귀
- 대표적인 회귀 알고리즘이다.
- 사이킷런은 sklearn.linear_model 패키지 아래에 LinearRegression 클래스로 선형 회귀 알고리즘을 구현해놓았다.
- 비교적 간단하고 성능이 뛰어나다.
- 특성이 하나인 경우 특성을 가장 잘 나타내는 직선을 학습하는 알고리즘이다.

### 회귀선 기울기와 산점도 방향성에 따른 결정계수
- 기울기를 계수(coefficent) 또는 가중치(weight)라고 함
1) 일직선인 경우 : 모든 데이터를 하나로 예측 (R<sup>2</sup> 0에 가까워짐)
2) 데이터와 반대 방향인 경우 : 완전히 반대로 예측 (R<sup>2</sup> 음수가 될 수 있음)
3) 데이터와 같은 방향인 경우 : 적절히 예측 (R<sup>2</sup> 1에 가까워짐)
   
### 파라미터
- 모델 파라미터 : 머신러닝 알고리즘이 찾은 값
- 모델 기반 학습 : 최적의 모델 파라미터를 탐색하는 방식, 이 책에서 사용하는 많은 머신러닝 알고리즘의 훈련과정은 모델기반 학습이다.
- 사례 기반 학습 : 모델 파라미터 없이 훈련 세트를 저장하는 것이 훈련의 전부, 앞서 사용한 k-최근접 이웃에는 모델 파라미터가 없으므로 사례 기반 학습이다.

<br>

## 다항 회귀<sup>polynomial regression</sup>
- 다항식을 활용한 선형 회귀
- 함수는 비선형일 수 있지만 선형 회귀로 표현 가능
- 선형 회귀보다 조금 더 복잡함
