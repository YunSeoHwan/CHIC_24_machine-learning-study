# Chapter3. 회귀 알고리즘과 모델 규제
#### - 지도 학습 알고리즘의 한 종류인 회귀 알고리즘에 대해 공부
#### - 다양한 선형 회귀 알고리즘의 장단점 이해

<br>

### 회귀
- 지도 학습 알고리즘은 크게 분류와 회귀로 나뉜다.
- 분류는 샘플을 몇 개의 클래스 중 하나로 분류하는 문제이고 회귀는 클래스 중 하나로 분류하는 것이 아니라 임의의 어떤 숫자를 예측하는 것이다.
- 임의의 수치를 예측하는 문제, 타깃값도 임의의 수치가 된다

### k-최근접 이웃 회귀
- k-최근접 이웃 알고리즘을 사용해 회귀 문제를 푼다. 가장 가까운 이웃 샘플을 찾고 이 샘플들의 타깃값의 평균을 구해 예측으로 삼는다.
- 사이킷런에서 k-최근접 이웃 회귀 알고리즘(모델)을 구현한 클래스는 KNeighborsRegressor이고 클래스 사용법은 앞에서 배운 KNeighborsClassifier와 매우 비슷하다.
  - 객체를 생성 후 fit()메서드로 회귀 모델 훈련
  
```
from sklearn.neighbors import KNeighborsRegressor
knr = KNeighborsRegressor()
knr.fit(train_input, train_target)
```

### 결정계수(R<sup>2</sup>)
- 대표적인 회귀 문제의 성능 측정 도구이다. (분류의 경우에는 점수를 '정확도'라고 부르는데 회귀에서는 '결정계수'라고 부름.)
-  1에 가까울수록 성능이 좋고 0에 가까울수록 성능이 나쁜 모델이다.
- R<sup>2</sup>= 1 - { (타깃-예측<sup>2</sup>의 합) / (타깃-평균<sup>2</sup>의 합) }
  - 각 샘플의 타깃과 예측한 값의 차이를 제곱하여 더한 후 타깃과 타깃 평균의 차이를 제곱하여 더한 값으로 나눔
  - 타깃의 평균 정도를 예측한다면 분자와 분모가 비슷해지기 때문에 R<sup>2</sup>는 0에 가까워짐
  - 예측이 타깃에 아주 가까워지면 분자가 0에 가까워지기 때문에 1에 가까워짐

<br>

## 과대적합과 과소적합
- ### 과대적합
훈련 세트에만 잘 맞게 학습된 모델
모델의 훈련 세트 성능이 테스트 세트 성능보다 훨씬 높을 때 일어난다.
모델이 훈련세트에 너무 집착해서 데이터에 내재된 거시적인 패턴을 감지하지 못한다. (테스트 세트 혹은 실제 적용 시 잘 동작하지 않음)

- ### 과소적합
과대적합과 반대로 테스트 세트 성능이 훈련 세트 성능보다 더 높을 때 일어나고 훈련세트와 테스트 세트 성능이 모두 동일하게 낮을 때도 일어난다.
모델이 너무 단순하여 훈련 세트에 적절히 훈련되지 않은 경우이다.
훈련 세트/테스트 세트 크기가 매우 작은 경우에도 과소적합이 발생한다.

- ### 과대적합과 과소적합 해결
  - 과소적합은 모델을 더 복잡하게 만들어야 함
    - k-최근접 이웃 알고리즘에서는 이웃의 개수 k를 줄여 모델을 복잡하게 만듦
      - 사이킷런의 k-최근접 이웃 알고리즘의 기본 k값은 5인데 3으로 낮추면 됨.
      - ```
        knr.n_neighbors = 3
        ```
      - 이웃의 개수를 줄이면 훈련 세트에 있는 국지적 패턴에 민감해짐
  - 과대적합은 모델을 덜 복잡하게 만들어야 함
    - k-최근접 이웃 알고리즘에서는 이웃의 개수 k를 늘려 모델을 단순하게 만듦  
      - 이웃의 개수를 늘리면 데이터 전반에 있는 일반적 패턴을 따름
