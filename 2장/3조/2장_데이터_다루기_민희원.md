# 2장 데이터 다루기 (2-1 ~ 2-2장)
2장을 학습하며 추가로 찾아본 내용과 함께 정리하였습니다.

- 1주차 학습 범위 (1-1, 26p ~ 2-1, 86p)
- 2주차 학습 범위 (2-2, 87p ~ 3-1, 129p)

## 지도 학습과 비지도 학습
### 지도학습
정답이 있는 데이터를 활용해 데이터를 학습시키는 것.
대표적으로 분류, 회귀 문제가 있음.

- 지도학습 종류
1. **분류 (Classification)** : 주어진 데이터를 정해진 카테고리(라벨)에 따라 분류하는 문제를 말함.

   맞다, 아니다 등의 **이진 분류 문제** 또는 사과, 바나나, 포도 등 2가지 이상으로 분류하는 **다중 분류 문제**가 있음.

   +) 분류는 종속변수를 단어, 명사형으로 정의할 수 있음. 대체로 종속변수가 숫자로 나오지 않는 것. 

2. **회귀 (Regression)** : 어떤 데이터들의 Feature를 기준으로, 연속된 값(그래프)을 예측하는 문제로 **주로 어떤 패턴이나 트렌드, 경향을 예측할 때 사용됨.** 즉 답이 정수처럼 딱 떨어지는게 아니고 어떤 수나 실수로 예측될 수 있음.

   +) 종속변수가 숫자로 나오면 대체로 회귀인 것이 많음. 수치적 예측.


### 비지도학습
**정답 라벨이 없는 데이터**를 비슷한 특징끼리 군집화하여 새로운 데이터에 대한 결과를 예측하는 방식.
대표적으로 클러스터링(Clustering), DBSCAN 등이 있음.

### +) 강화학습 (Reinforcement Learning)
행동 심리학에서 나온 이론으로 분류할 수 있는 데이터가 존재하는 것도 아니고 데이터가 있어도 정답이 따로 정해져 있지 않으며 **자신이 한 행동에 대해 보상(reward)를 받으며 학습하는 것**을 말함.

이전부터 존재했던 학습법이지만, 이전 알고리즘은 실생활에 적용할 수 있을만큼 좋은 결과를 내지 못했음. 하지만 **딥러닝의 등장 이후 강화학습에 신경망을 적용**하면서부터 바둑이나 자율주행차와 같은 복잡한 문제에 적용할 수 있게 됨.

+) 학습 방향은 행동에 대한 보상이 큰 쪽으로 학습을 진행함.

## 샘플링 편향(sampling bias)
샘플링 편향은 관심 모집단의 일부 구성원이 다른 구성원보다 샘플링 확률이 낮거나 높은 방식으로 샘플을 수집하는 편향임.

해결 방법으로는 **train_test_split()** 메서드가 존재. **stratify** 매개변수에 타깃 데이터를 전달하면 클래스 비율에 맞게 데이터를 나눔.

ex) train_test_split(stratify=fish_target)

train data 가 작거나 특정 클래스의 샘플 개수가 적을 때 특히 유용함. 데이터가 작으면 전체 훈련 데이터의 비율과 동일하게 맞출 수는 없지만 비슷한 비율로 맞춰줌.

## 데이터 스케일링(Data Scaling)
### 데이터 전처리란?
거리 기반 알고리즘들은 샘플 간의 거리에 영향을 많이 받으므로 제대로 사용하려면 특성값을 일정한 기준으로 맞춰주어야 함. 이 작업을 데이터 전처리라고 부름.

가장 널리 사용하는 전처리 방법 중 하나는 **표준점수**(Standard score, 혹은 z 점수)임. 각 특성값이 평균에서 표준 편차의 몇 배만큼 떨어져 있는지를 나타냄.

### 브로드캐스팅(Broadcasting)
크기가 다른 넘파이 배열에서 자동으로 사칙 연산을 모든 행이나 열로 확장하여 수행하는 기능.

**넘파이에서 서로 다른 모양(shape)의 배열도 일정 조건을 만족하면 연산**할 수 있는데, 이 기능을 브로드캐스팅이라고 함.

**브로드캐스팅 조건**
- 원소가 하나인 배열은 어떤 배열이나 브로드캐스팅이 가능
- 하나의 배열이 1차원 배열인 경우, 브로드캐스팅이 가능.

  arr1.shape = (3, )

  arr2.shape = (3, 3)

  np.add(arr1, arr2) # 가능
- 차원의 짝이 맞을 때 브로드캐스팅이 가능

  arr1.shape = (4, 1)

  arr2.shape = (1, 4)

  np.add(arr1, arr2) # 가능

### 스케일링이란?
특성별로 데이터의 스케일이 다르다면, 머신러닝이 잘 동작하지 않을 수 있음. 따라서, 스케일링 작업을 통해 모든 특성의 범위(또는 분포)를 같게 만들어줘야 함.

### 주로 사용되는 스케일링 개념
- **Standardization (표준화)** : 특성들의 평균을 0, 분산을 1로 스케일링 하는 것. 즉, 특성들을 정규분포로 만드는 것.
- **Normalization (정규화)** : 특성들을 특정 범위(주로 [0, 1])로 스케일링 하는 것. 가장 작은 값은 0, 가장 큰 값은 1로 변환되므로, 모든 특성들은 [0, 1] 범위를 갖게 됨.

### scikit-learn의 scaler 사용 전, 주의 사항
- scaler는 fit과 transform 메서드를 지니고 있음.
- fit 메서드는 훈련 데이터만 적용해, 훈련 데이터의 분포를 먼저 학습하고
- 그 이후, transform 메서드를 훈련 데이터와 테스트 데이터에 적용해 스케일을 조정해야 함.
- **따라서 훈련 데이터에는 fit_transform() 메서드를 적용하고, 테스트 데이터에는 transform() 메서드를 적용해야 함.** fit_transform()은 fit과 transform이 결합된 단축 메서드임.

### 추가적인 스케일링 method
**1. StandardScaler()**
   - 특성들의 평균을 0, 분산을 1로 스케일링 하는 것. (즉, 특성들을 정규분포로 만드는 것.)
   - 최솟값과 최댓값의 크기를 제한하지 않기 때문에, **어떤 알고리즘에서는 문제가 있을 수 있으며 이상치에 매우 민감함.**
   - 회귀보다 **분류에 유용.**

**2. MinMaxScaler()**
   - **Min-Max Normalization**이라고도 불리며, 특성들을 특정 범위(주로 [0, 1])로 스케일링 하는 것임. **이상치에 매우 민감함.**
   - 분류보다 **회귀에 유용.**

**3. MaxAbsScaler()**
   - 각 특성의 절댓값이 0과 1 사이가 되도록 스케일링함.
   - 즉, 모든 값은 -1과 1 사이로 표현되며, 데이터가 양수일 경우 MinMaxScaler와 같음.
   - **이상치에 매우 민감함.**

**4. RobustScaler()**
   - 평균과 분산 대신에 **중간 값**과 **사분위 값**을 사용. **중간 값**은 정렬 시 중간에 있는 값을 의미하고, **사분위 값**은 1/4, 3/4에 위치한 값을 의미.
   - **이상치 영향을 최소화할 수 있음.**

**5. Normalizer()**
   - 위에서 언급한 Scaler는 각 특성(열)의 통계치를 이용하여 진행됨. 그러나 이 방식은 **각 샘플(행)마다 적용되는 방식임.**
   - 한 행의 모든 특성들 사이의 **유클리드 거리**(two norm)가 1이 되도록 스케일링함.
   - 일반적인 데이터 전처리 상황에서 사용되는 것이 아니라, 모델(특히 딥러닝) 내 학습 벡터에 적용하며 feature들이 다른 단위(키, 나이, 소득 등)라면 더더욱 사용하지 않음.