### [🔗 2-2. 데이터 전처리 Colab](https://colab.research.google.com/drive/1CeuMTCGdlYYBBBT4l6dQmaJsu-6cfoqM?usp=sharing)   
---
데이터 전처리 : 모델에 훈련 데이터를 주입하기 전 가공하는 단계

표준점수 : 훈련 세트의 스케일을 바꾸는 방법, 각 데이터가 원점에서 몇 표준편차만큼 떨어져 있는지를 나타내는 값

**훈련 세트**의 평균과 표준차로 테스트 세트를 바꿔야함

브로드캐스팅 : 크기가 다른 넘파이 배열에서 자동으로 사칙 연산을 모든 행이라 열로 확장해 수행

---
scikit-learn
- train_test_split()
  : 훈련 데이터를 훈련 세트와 테스트 세트로 나누는 함수, test_size 로 테스트 나눌 비율 지정 가능, 기본값 : 25%

- kneighbors() : K-최근접 이웃 객테의 메서드, 입력한 데이터에 가장 가까운 이웃을 찾아 **거리와 이웃 샘플의 인덱스** 반환

--- 
실습 설명
### 넘파이로 데이터 준비
```python
# 리스트 내포 대신 넘파이 .column_stack() 사용
import numpy as np
fish_data = np.column_stack((fish_length, fish_weight))
fish_data

# 타깃 데이터 만들기 np.concatenate() 그대로 연결
fish_target = np.concatenate((np.ones(35), np.zeros(14)))
# 출력 : [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0.]
```
### 사이킷런으로 세트 나누기
```python
# train_test_split() : 나누고싶은 배열을 원하는 만큼 전달
train_input, test_input, train_target, test_target = train_test_split(
    fish_data, fish_target, random_state=42)
# fish_data는 train_input, test_input 으로, fish_target은 train_target, test_target 으로
```
무작위로 나눴을 때 샘플이 골고우 섞이지 않을 수 있음

-> **stratify 매개변수에 타깃 데이터 전달하면 해결**
```python
# stratify=fish_target 지정
train_input, test_input, train_target, test_target = train_test_split(
    fish_data, fish_target, stratify=fish_target, random_state=42)
```

### 기준을 맞춰라
```python
# x축, y축 범위 동일하게 맞추기
plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(25, 150, marker='^')
plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')
plt.xlim((0, 1000))
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```
![image](https://github.com/SYEONIH/CHIC_24_machine-learning-study/assets/113624562/3e0fad6b-d175-466b-a071-66dbe42b64b1)

```python
# mean : 평균, std : 표준편차
mean = np.mean(train_input, axis=0)
std = np.std(train_input, axis=0)
print(mean, std)
# 출력 : [ 27.29722222 454.09722222] [  9.98244253 323.29893931]

# 표준점수 : 각 데이터가 원점에서 몇 표준편차만큼 떨어져 있는지를 나타내는 값
train_scaled = (train_input - mean) / std
```


### 전처리 데이터로 모델 훈련하기
```python
# 표준 점수로 반환 -> 범위가 달라서 이상함 ( 샘플 [25, 150] 동일 비율로 반환 필요 )
plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(25, 150, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```
![image](https://github.com/SYEONIH/CHIC_24_machine-learning-study/assets/113624562/551c5e07-06cb-4447-8831-e9adc71a7349)

```python
# 샘플 [25, 150] 동일 비율로 반환
new = ([25, 150] - mean) / std

plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(new[0], new[1], marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```
![image](https://github.com/SYEONIH/CHIC_24_machine-learning-study/assets/113624562/6e960acc-6170-43ca-8e93-74a351bc21a1)

```python
kn.fit(train_scaled, train_target)
test_scaled = (test_input - mean) / std
kn.score(test_scaled, test_target)

# 도미로 예측 : [1.]
print(kn.predict([new]))
```
```python
# 시각화
distances, indexes = kn.kneighbors([new])

plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(new[0], new[1], marker='^')
plt.scatter(train_scaled[indexes,0], train_scaled[indexes,1], marker='D')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```
![image](https://github.com/SYEONIH/CHIC_24_machine-learning-study/assets/113624562/6bba84fd-90fb-44b9-bf6e-a945690c8076)

---
## 확인 문제
1. 이 방식은 스케일 조정 방식의 하나로 특성값을 0에서 표준편차의 몇 배수만큼 떨어져 있는 지로 변환하는 값입니다. 이 값을 무엇이라 부르나요?<details>**표준점수**<summary>정답
</summary></details>

2. 테스트 세트의 스케일을 조정하려고 합니다.  다음 중 어떤 데이터의 통계 값을 사용해야 하나요?<details>**훈련 세트**<summary>정답
</summary></details>
