# 점진적 학습(온라인 학습)
- 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 조금씩 훈련할 수 있는 훈련 방식
- 대표적인 점진적 학습 알고리즘 : 확률적 경사 하강법

# 확률적 경사 하강법
: 훈련 세트에서 샘플 하나씩 꺼내 손실 함수의 경사를 따라 최적의 모델을 찾는 알고리즘
- 신경망 알고리즘 : 확률적 경사 하강법을 꼭 사용하는 알고리즘, 미니배치 경사 하강법도 사용함
- SGDClassifier : 사이킷런에서 확률적 경사 하강법을 제공하는 대표적인 분류용 클래스
  - sklearn.linear_model 패키지 아래에서 임포트
  - 매개변수
    - loss : 손실 함수의 종류를 지정
      - log : 로지스틱 손실 함수, 다중 분류일 경우 클래스마다 이진 분류 모델을 만듦
              ex) 도미를 양성 클래스로 두고 나머지르 모두 음성 클래스로 두고 모델을 만듦 => Ovr 방식
      - hinge : 힌지 손실, 서포트 벡터 머신이라고도 불림, loss 매개변수의 기본값
      - 위의 2가지 외에도 여러 종류의 손실 함수를 loss 매개변수에 지정해 다양한 머신러닝 알고리즘을 지원함
    - max_iter : 수행할 에포크 횟수를 지정
    - tol : 향상 될 최솟값 지정, None 지정시 자동으로 멈추지 않고 max_iter 값만큼 무조건 반복
  - partial_fit() : 확률적 경사 하강법은 점진적 학습이 가능 → 이 함수를 이용해 모델을 이어서 훈련
    - fit() 메서드와 사용법이 같지만 호출할 때마다 1 에포크씩 이어서 훈련 가능
+)
- 미니배치 경사 하강법 : 샘플을 하나씩 사용하지 않고 여러 개 사용시
- 배치 경사 하강법 : 한 번에 전체 샘플 사용
- SGDRegressor : 확률적 경사 하강법을 사용한 회귀 모델을 만듦
  - loss 매개변수에서 손실 함수 지정
    - 기본값은 제곱 오차를 나타내는 squqred_loss
  - SGDClassfier에서 설명한 매개변수 모두 동일하게 사용됨
# 손실 함수
: 어떤 문제에서 머신러닝 알고리즘이 얼마나 엉터리인지를 측정하는 기준
- 샘플 하나에 대한 손실을 정의
- 확률적 경사 하강법이 최적화할 대상
- 분류에서 손실 = 정답을 못 맞히는 것
- 손실함수는 미분 가능해야함 → 연속적이여야 함
- 대부분의 문제에 잘 맞는 손실 함수가 이미 정의되어 있음
  - 이진 분류의 경우 : 로지스틱 회귀(또는 이진 크로스엔트로피) 손실 함수 사용
  - 다중 분류의 경우 : 크로스엔트로피 손실 함수 사용
  - 회귀 문제의 경우 : 평균 제곱 오차 손실 함수 사용

# 에포크
: 확률적 경사 하강법에서 전체 샘플을 모두 사용하는 한 번 반복을 의미
- 일반저긍로 경사 하강법 알고리즘은 수십에서 수백 번의 에포크 반복
- 확률적 경사 하강법을 사용한 모델은 에포크 횟수에 따라 과소적합이나 과대적하이 될 수 있음
  - 과소적합의 경우 : 적은 에포크 횟수 → 모델이 훈련 세트를 덜 학습 → 과소적합된 모델일 가능성 높음
  - 과대적합의 경우 : 많은 에포크 횟수 → 훈련 세트를 완전히 학습, 훈련 세트에 아주 잘 맞는 모델 → 과대적합의 가능성 높음
  - 조기 종료 : 과대적합이 시작되기 전에 훈련을 멈추는 것
    - 훈련 세트 점수는 에포크가 진행될수록 꾸준히 증가하지만 테스트 세트 점수는 어느 순간 감소하기 시작 = 이 지점이 과대적합되기 시작하는 곳
    -
