* 타깃값을 그대로 사이킷런 모델에 전달하면 순서가 자동으로 알파벳 순서로 매겨짐.

# k-최근접 이웃 분류기의 확률 예측
- if 3개의 최근접 이웃을 사용한다고 가정
→ 가능한 확률은 0/3, 1/3, 2/3, 3/3이 전부
→ 확률을 이렇게 표시시 확률이라고 말하기 어색 할 수 있음
→ 로지스틱 회귀를 사용해보자!

# 로지스틱 회귀
: 이름은 회귀이지만 선형 방정식을 사용한 분류 알고리즘 즉 분류 모델임. 
- 선형 회귀와 동일하게 선형 방정식을 학습
- 선형 회귀와 달리 시그모이드 함수나 소프트맥스 함수를 사용해 클래스 확률을 출력
- LogisticRegression :  사이킷런에 있는 선형 분류 알고리즘인 로지스틱 회귀를 위한 클래스
- predict_proba() 함수 : 예측 확률을 반환함
- decision_function() : 모델이 학습한 선형 방정식의 출력을 반환
  - 이진 분류 : 양성 클래스의 확률이 반환 됨, 이 값이 0보다 크면 양성 클래스, 작거나 같으면 음성 클래스로 예측
  - 다중 분류 : 각 클래스마다 선형 방정식 계산하고 가장 큰 값의 클래스가 예측 클래스 됨
- 시그모이드 함수(로지스틱 함수) 
  - 파이썬의 사이파이 라이브러리에 시그모이드 함수 존재, scipy.special 아래 expit() 함수 임포트해 사용
  - 선형 방정식의 출력을 0과 1사이의 값으로 압축해 이진 분류를 위해 사용
  - 선형 방정식의 출력 z의 음수를 사용해 자연 상수 e를 거듭제곱하고 1을 더한 값의 역수를 취함
  - z가 무한하게 큰 음수일 경우 이 함수는 0에 가까워지고, z가 무한하게 큰 양수가 될 때는 1에 가까워짐
    → 즉 z가 0이 될 때는 0.5가 됨, 어떤 값이 되더라도 절대로 0~1 사이으 범위를 벗어날 수 없음
      (*사이킷런에서는 정확히 0.5일 경우 음성 클래스로 판단함)
  * 지수 함수 계산은 np.exp() 함수 사용 
- 다중 분류 : 타깃 클래스가 2개이상인 분류 문제, 로지스틱 회귀는 다중 분류를 위해 소프트맥스 함수를 사용해 클래스 예측함
  - 다중 분류는 클래스마다 z 값을 하나씩 계산, 가장 높은 z값ㄹ을 출력하는 클래스가 예측 클래스가 됨
- 소프트맥스 함수 
  - 다중 분류에서 선형 방정식의 출력 결과를 정규화해 합이 1이 되도록 만듦
  - 지수 함수를 사용해 정규화된 지수 함수라고도 불림.
  - scipy.special 아래에 softmax() 함수 임포트해 사용
  - softmax()의 axis 매개변수는 소프트맥스를 계산할 축을 지정
    - softmax(decision, axis = 1) : axis = 1로 지정시 각 행, 즉 각 샘플에 대한 소프트맥스를 계산
      - axis 매개변수를 지정하지 않으면 배열 전체에 대해 소프트맥스를 계산함
