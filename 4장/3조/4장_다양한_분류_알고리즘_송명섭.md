240201 스터디 정리

(이번에는 의식의 흐름대로 정리했다.)

경사하강법의 종류와 의의를 이해하기 전에, 근본적으로 다시 생각해보자. 왜 할까? \
Q. Why GD? \
A. Optimization! \
- 파라미터를 조정(후술할 iterative, 시행착오에 따른 업데이트)하면서 \
loss를 최소화하는, 그 최적의 파라미터를 탐색
- loss function(loss 또는 cost)의 기울기(그래디언트)를 따라 내려가면서 최솟값에 도달

+ 참고) 그래디언트와 방향도함수 \
  그래디언트는 변수들의 편미분값을 모아놓은 벡터 \
  방향도함수는 특정 방향으로의 변화율(스칼라)
![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/129747097/2f05b796-e7b5-49fd-9294-f53a00f1ceb7)

그래디언트는 상수니까 스칼라 아닌가 질문했다가 떠올려보니 그건 방향도함수였다.\
사실 매우 비중있지는 않지만 왜 그래디언트가 가장 가파른 방향을 가리키는지에 대한 수학적, 직관적 이해를 원했다.\
요약하면 \
크기만을 갖는 스칼라함수 f에 ∇를 붙이면 \
크기 뿐만 아니라 방향까지 갖는 그래디언트 ∇f가 된다. \ 
방향은 가장 가파른 방향, 크기는 그 방향으로의 기울기값이 된다. \

Q. 최대 혹은 최소를 탐색하는데, r그냥 f'(x)=0인 지점을 바로, direct로 찾으면 안 돼?
A. 물론 함수가 간단하면 그렇게도 찾을 수는 있을 것이다. \
하지만 경사하강법은 바로 찾는 것이 아닌, 업데이트 과정을 통해 점진적으로 최적의 지점에 도달하는 방법이다. \
실제 분석에서 접하게 될 함수들은 닫힌 형태 or 비선형 등으로 미분계수와 근을 찾기 난해하며, \
데이터가 많을 때는 GD가 더 효율적이다. (다 이유가 있으니까 배우는 거다. 하지만 공부 단계니 따져보는 건 좋다.) \

\
그렇다면 이제 손실을 정의해보자. \
'오차'라 하면 예측과 실제 간 차이지만, 수식적인 정의는 다양하다. \
MSE(제곱합의 평균), MAE(절댓값합의 평균) 외에도 \
Cross-Entrophy가 있다.(후에 추가하겠다.)

![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/129747097/c927869c-4fa0-483d-b01b-cff87d111cec)
   
여기서 고려해야 할 것이, 학습 단위이다.\
기울기 계산을 반복한다는 것은 파라미터 업데이트도 반복된다는 것이고, \
이때 수많은 데이터를 모델에 담은 채 진행을 할 것이다. If too much, 과부하 발생 \

![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/129747097/ea78f17d-315c-495c-b829-75eefe02fb61)

경사하강법의 종류는 배치 당 담긴 데이터의 수로 구분한다.

1. GD
- 쉽게 말해, 데이터를 싹 다 때려박아 한 번에 돌리는 것이다.
- 당연히 모든 데이터를 썼으니 원하는 값으로 안정적으로 수렴할 것이다.
- 과부하, 즉 메모리 문제가 따라온다.
- loss minimum 문제도 있다.

2. SGD
-  위의 문제들을 해결하기 위해 활용한다. 사실 해소하는 건 아니고, \
-  발생 가능성을 낮춘다는 게 맞겠다. 100%라는 건 없다.
-  데이터를 1개씩만 뽑아서 파라미터를 조정에 활용한다.
-  등고선을 보면 상당히 중구난방으로 이동하는 듯 보이는데, \
   실질적으로는 데이터 1개만 쓰니 GD보다 효율적이다.

3. mini-batch
- 이건 개수를 지정할 수 있다고 한다. 종류보단 전체적인 기능을 보자. 
