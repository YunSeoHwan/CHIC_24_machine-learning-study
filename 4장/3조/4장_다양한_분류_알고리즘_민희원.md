# 4장 다양한 분류 알고리즘 
4주차 스터디 범위 (p.175 ~ 241)

## 04-1 로지스틱 회귀
### 다중 분류 (Multi-class classification)
target data에 2개 이상의 클래스가 포함된 문제

>ex) 7개의 생선 종류

타깃 값을 그대로 사이킷런 모델에 전달하면 순서가 자동으로 알파벳 순서로 매겨짐. 따라서 pd.unique()로 출력한 순서와는 다름. 정렬된 타깃값은 `classes_` 속성에 저장되어 있음.

### 로지스틱 회귀
- 이름은 회귀이지만 **분류 모델**임.
- 해당 알고리즘은 선형 회귀와 동일하게 선형 방정식을 학습함.
- **`시그모이드 함수`** sigmoid function (또는 **`로지스틱함수`** logistic function)를 사용하여 확률값인 0~1(또는 0 ~ 100%) 사이 값으로 바꿈.

> ### 활성화 함수
> **`sigmoid 함수`, `ReLu 함수`, `softmax 함수`**

### 시그모이드 함수 (Sigmoid function)
- 사이파이 라이브러리(scipy)에도 시그모이드 함수 `expit()` 존재.
- 입력값이 커지면 커질수록 1에 수렴하고, 작아질수록 0에 수렴.
- 미분하면, 양 쪽으로 향할수록 변화값이 거의 없음.
- 오류역전파를 할 때, Vanishing Gradient 현상 발견될 수 있음.
- 0 또는 1을 반환하기 때문에, **이진분류모델**의 마지막 활성화 함수로 사용됨.

### ReLu 함수
- **은닉층**의 활성화 함수로 사용됨.
- 입력값이 0보다 작거나 같을 때는 항상 0을 출력하고, 0보다 크면 입력값과 동일한 출력값을 출력.

### 소프트맥스 함수 (softmax function)
- 시그모이드와 비슷하게 0~1 사이로 변환하여 출력하지만, 출력값들의 합이 1이 되도록 하는 함수
- **다중분류**의 최종 활성화 함수로 사용됨.

## 04-2 확률적 경사 하강법
### 점진적 학습 (또는 온라인 학습)
대표적인 점진적 학습 알고리즘은 **확률적 경사 하강법**이 있음.

### 확률적 경사 하강법 (Stochastic Gradient Descent)
훈련 세트에서 랜덤하게 하나의 샘플을 고르는 것