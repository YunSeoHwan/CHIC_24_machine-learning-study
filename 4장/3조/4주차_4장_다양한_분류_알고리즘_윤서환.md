# 다양한 분류 알고리즘(4-1, 4-2) - 4주차
**4-2**까지 내용을 정리함을 서두에 밝힙니다.<br>
책 내용에서 보다 깊이 있게 다루면 좋을 요소 위주의 정리를 진행했습니다. 따라서 책 전체 내용의 기술되어 있지 않을 수 있습니다. 궁금한 사항이나, 잘못된 부분이 있다면, 언제든 issue에 의견을 남겨주시면 감사하겠습니다. 해당 내용은 **데이터 마이닝  - 이상민 교수님**자료를 참고하여 만들었습니다.

## Sigmoid Function(시그모이드 함수)
일반적으로 Binary Classification 문제에서 Y값이 0, 1로 표현되기 때문에 이를 처리 과정 없이 사용하게 되면 아래와 같은 문제가 발생할 수 있습니다.
![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/48356954/0a3d4c31-4982-48b9-8f22-19f20afa7dcb)
<br><br>
따라서 이와 같은 문제를 해결하기 위해 최종 출력을 **Sigmoid**를 이용하여 나타나게 됩니다. Sigmoid를 이용하면 0 ~ 1 사이의 확률값으로 mapping할 수 있습니다. 대표적으로 로지스틱(Logistic) 함수가 있습니다. 그렇다면 왜 sigmoid를 사용할까요? 바로 **Monotonic function(단조 함수)** 이기 때문입니다. 이러한 Monotonic의 특징은 아래와 같습니다.
![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/48356954/40968349-b99c-4222-80d4-1c06a0fe80a4)
<br><br>

## Batch, iter, epoch
### Batch
Batch의 사전적 의미는 (일괄적으로 처리되는)집단입니다. Batch는 한번에 여러개의 데이터를 묶어서 입력하는 것인데, GPU 의 병렬 연산 기능을 최대한 효율적으로 사용하기 위해 쓰는 방법입니다. batch_size는 보통 1개의 batch에 속한 데이터의 갯수를 의미합니다.
<br>

### Epoch
전체 dataset을 1회 학습한 것을 **1 epoch** 으로 정의합니다. epoch이 10 이라면, 전체 dataset을 10번 학습한 것과 동일합니다.
<br>

### Iter
**1 epoch을 하기 위해 필요한 반복 횟수**를 의미합니다. 만약 100개의 data가 존재하고 batch가 1 이라면 1 epoch을 하기 위해서는 100번의 iteration이 필요합니다. 즉, batch size와 data의 갯수에 따라 iteration은 변하게 됩니다.
<br>

## Batch gradient descent(BGD or GD)
일반적으로 알고 있는 GD와 동일합니다. 다만, 여기서 batch라는 용어를 사용한 것은 이 당시에는 batch라는 개념이 명확히 잡혀있지 않았습니다. 대부분 전체 data를 모두 넣고 학습하는 방식을 택했습니다. 해당 방식의 장단은 아래와 같습니다.

### 장점
- 전체 데이터에 대해 업데이트가 한번에 이루어지기 때문에 후술할 SGD 보다 업데이트 횟수가 적다. 따라서 전체적인 계산 횟수는 적습니다.
- 전체 데이터에 대해 error gradient 를 계산하기 때문에 optimal한 수렴이 안정적으로 진행됩니다.

### 단점
- 한 스텝에 모든 학습 데이터 셋을 사용하므로 학습이 오래 걸립니다.
- 전체 학습 데이터에 대한 error 를 모델의 업데이트가 이루어지기 전까지 축적해야 하므로 더 많은 메모리가 필요합니다.
- local optimal 상태가 되면 빠져나오기 힘듦니다.
<br><br>

## Stochastic gradient descent(SGD)
추출된 데이터 한 개에 대해서 Gradient descent 알고리즘을 적용하는 방법입니다. 즉, batch size를 1로 가져가는 방식이라 볼 수 있습니다. 해당 방식의 장단은 아래와 같습니다.
![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/48356954/f7e5c29f-f63b-4f9d-81fd-ce36e30abf2a)

### 장점
- 위 그림에서 보이듯이 Shooting이 일어나기 때문에 local optimal 에 빠질 리스크가 적습니다.
- Step에 걸리는 시간이 짧기 때문에 수렴속도가 상대적으로 빠릅니다.

### 단점
- global optimal 을 찾지 못 할 가능성이 있습니다.
- 데이터를 한개씩 처리하기 때문에 GPU의 성능을 전부 활용할 수 없습니다.
<br><br>

## Mini-batch gradient descent(MSGD)
batch size가 1보다 크게 가져가 SGD 방식으로 학습하는 방식입니다. 해당 방식의 장단은 아래와 같습니다.
![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/48356954/62d26089-7055-418d-be33-faa828426c26)

### 장점
- BGD보다 local optimal 에 빠질 리스크가 적습니다.
- SGD보다 병렬처리에 유리합니다.
- 전체 학습데이터가 아닌 일부분의 학습데이터만 사용하기 때문에 메모리 사용이 BGD 보다 적습니다.

### 단점
- batch size(mini-batch size) 를 설정해야 합니다.
- 에러에 대한 정보를 mini-batch 크기 만큼 축적해서 계산해야 하기 때문에 SGD 보다 메모리 사용이 높습니다.
