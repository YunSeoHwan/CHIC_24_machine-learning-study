### <네 번째 스터디 – 2024.02.01> 
### 범위: p.176~p.241
- 질의응답 중심으로 스터디를 진행하였습니다.
### Chapter 4 다양한 분류 알고리즘
##### 시그모이드 함수(로지스틱 함수)
- 시그모이드: 기울어진 S자 형태의 곡선
- 단조함수이고, 단 하나의 변곡점
- monotonic fuction이다 : 항상 미분값이 양의 값을 가지고 미분 가능.
- 단점: x가 일정 이상이면 기울기의 차이가 없기때문에 큰 차이가 없어진다. (분모부분이 지수함수형이므로) Vanishing Gradient가 발생할 수 있다.
- -> 해결을 위해 랠로 함수를 사용하기도 함. 
※ 랠로 함수 
f(x) = x (x>0)
0(x<=0)
##### Batch와 epoch 그리고 iter
Batch: 한번에 처리하는 집단. 한번에 여러개의 데이터를 묶어서 입력하는 것.
- GD: 모든 데이터를 하나의 batch로 잡은 것 (batch의 개념이 없었음)
- SGD: batch size를 1로 가져감(무작위)
- MSGD : batch size를 k개 (예를들어 5개..)
epoch: 전체 데이터셋을 1회 학습한 것
iter : 1epoch를 위해 필요한 반복의 횟수 
##### 손실함수(loss function)
- 최소화의 대상 / cost function
- 손실함수가 미분가능해야 경사 하강법을 사용할 수 있음.
- loss를 어떻게 설계할 것인가? -> loss는 모델을 검증하기 위해 필수적인 것,
- 분류에서는 정답을 못 맞추는 것이 loss이다.
- min Loss 이냐 max Loss 이냐는 loss fuction에 영향을 받음


##### GD(Gradient Descent)
- 장점: 전체 데이터에 대해 한번에 업데이트 되므로 업데이트 횟수가 적다. global optimal를 찾을 수 있다
- 단점: 학습이 오래 걸린다. 많은 메모리(resource)가 필요하다.
##### SGD(Stochastic Gradient Descent)
- 장점: 한번의 stop에 소요되는 시간이 적다
- 단점: global optimal을 찾지 못할 수 있다. 
##### MSGD(Mini-Batch Stochastic Gradient Descent)
- 장점: 메모리 사용이 GD보다 적다. GD보다 local optimal에 빠질 리스크가 적다
- 단점: batch size를 설정해야 하므로 찾게되는 optimal point가 다를 수 있다. SGD보다 많은 메모리 resource를 사용하게 된다.

##### 양성 클래스와 음성 클래스의 구분
- 임계값보다 크면 양성클래스, 아니면 음성클래스
- stakeholder에 따라 결정
##### 정확도(score)이외의 지표
- <분류> recall/f1score
- <회귀> R^2 / MAE 등등

