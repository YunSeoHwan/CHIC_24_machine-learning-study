# 용어
- 다중 분류 : 타깃 데이터(target data)에 2개 이상의 클래스가 포함된 분류 문제. 소프트맥스 함수를 사용하여 클래스 예측
- numpy : 배열
- 소프트맥스 함수 : 여러 개의 선형 방정식의 출력값을 0~1 사이로 압축하고 전체합이 1 이 되도록 만드는 것
- 로지스틱 회귀 : 선형 방정싱을 사용한 분류 알고리즘. 선형회귀와 달리 시그모이드 함수나 소프트맥스 함수를 사용하여 클래스 확률 출력
- 시그모이드 함수 : 선형 방정식의 출력을 0과 1사이의 값으로 압축하여 이진 분류를 위해 사용
- 점진적 학습(온라인 학습) : 앞서 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 더 훈련
- 확률적 경사 하강법 : 훈련 세트에서 샘플 하나씩 꺼내 손실 함수의 경사를 따라 최적의 모델을 찾는 알고리즘.   샘플을 하나씩 사용하지 않고 여러 개를 사용하면 미니배치 경사 하강법이 된다. 한 번에 전체 샘플을 사용하면 배치 경사 하강법이 된다.
- 손실 함수 : 확률적 경사 하강법이 최적화할 대상
- Batch Size : 전체 데이터 셋을 여러 작은 그룹으로 나누었을떄, 하나의 소그룹에 속하는 데이터 수를 의미
> Batch size가 너무 큰 경우, 한 번에 처리해야할 데이터의 양이 많아져 학습 속도가 느려지고 메모리 부족이 발생할 수 있다.

> Batch size가 너무 작은 경우, 적은 데이터로 가중치가 자주 업데이트되어 훈련이 불안정해진다
- Epoch : 딥러닝에서 epoch는 전체 데이터 셋이 신경망을 통과한 횟수. 즉, 모든 데이터셋을 학습하는 횟수    
  (확률적 경사 하강법에서 전체 샘플을 모두 사용하는 한 번 반복 의미)
> 1-epoch는 전체 데이터셋이 하나의 신경망에 적용되어 순전파와 역전파를 통해 신경망을 한 번 통과했다는 것을 의미
> Epoch 값이 너무 작으면 Underfitting, 너무 크면 Overfitting 아 발생할 확률이 높다
- Iteration : 1-epoch를 마치는데 필요한 미니배치의 수(1-epoch를 마치는데 필요한 파라미터 업데이트 횟수)   
> 2000개의 데이터를 200개씩 10개의 미니배치로 나눈다면, 1-epoch을 위해선 10-iteration이 필요하며 10번의 파라미터 업데이트가 진행된 것
   
> 2000개의 데이터에 batch_size = 500 일 시 epoch을 20번으로 설정한다면, iteration이 4번이 될 것이고 전체 데이터 셋에 대해선 총 20번의 학습이 이루어졌으며, iteration 기준으론 80번의 학습이 이루어진 것이다.

<img src = "https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/73346564/cc2c1db1-a3d7-4468-9fd8-c69de843eaa8" width="70%"></img>

# 4-1 로지스틱 회귀
: 로지스틱 회귀(Logistic Regressoin)는 이름은 회귀이지만 분류 모델이다.   
선형 회귀와 동일하게 선형 방정식을 학습한다.

> #### 시그모이드 함수에서 출력이 0.5 이면?
> 정확히 0.5일때, 사이킷런은 음성 클래스로 판단한다.

- 넘파이 배열은 True, False 값을 전달하여 행을 선택할 수 있다.
```
ex. 'A' 에서 'E'까지 5개의 원소로 이루어진 배열에서 'A'와 'C'만 골라내는 법
char_arr = np.array(['A', 'B', 'C', 'D', 'E'])
print(char_arr[[True, False, True, False, False]])

-> ['A', 'C']
```
#### 로지스틱 회귀(Logistic Regression)
- 기본적으로 반복전인 알고리즘 사용
- max_iter 매개변수에서 반복 횟수를 지정하면 기본값은 100
- 릿지 회귀와 같이 계수의 제곱을 규제(L2 규제)
- 릿지 회귀에서는 alpha 매개변수로 규제의 양 조절, alpha가 커지면 규제도 증가
- Logistic Regression에서 규제 제어하는 매개변수는 C, C는 작아질수록 규제 증가. 기본값은 1

#### 로지스틱 회귀를 사용하여 다중 분류
##### [가정, 각 샘플마다 5개의 특성을 가지고 있으며, 총 7개의 샘플이 있다.]
- 5개의 특성을 사용하므로, coef_ 배열의 열은 5 이다
- intercept_ 는 7 이다. 즉, 다중분류는 클래스마다 z값을 하나씩 계산하며 가장 높은 z값을 출력하는 클래스가 에측 클래스이다.

> #### 소프트맥스 함수
> 시그모이드 함수는 하나의 선형 방정식의 출력값을 0 ~ 1 사이로 압축한다. 이와 달리 소프트맥스 함수는 여러 개의 선형 방정식의 출력값을 0 ~ 1 사이로 압축하고   
> 전체 합이 1이 되도록 만든다. 이를 위해 지수 함수를 사용하기 때문에 정규화된 지수 함수라고도 부른다.

# 4-2 확률적 경사 하강법
: 가장 가파른 경사를 따라 원하는 지점에 도달하는 것이 목표이다.   
하지만, step이 길다면 경사를 따라 내려가지 못하고 오히려 올라갈 수 있다.
<img src = "https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/73346564/3419c4e7-db35-4201-b1f9-735ecf2fdd04" width="70%"></img>
#### 가장 가파른 길을 조금씩 내려오는 것이 중요하다  

- 확률적 경사 하강법에서 훈련 세트를 한 번 모두 사용하는 과정을 에포크(epoch) 라고 부른다.
- 일반적으로 경사 하강법은 수십, 수백 번 이상의 에포크를 수행한다.

#### 미니배치 경사 하강법
: 여러개의 샘플을 사용해 경사 하강법을 수행하는 방식

#### 배치 경사 하강법
: 한 번에 전체 샘플을 사용  
전체 데이터를 사용하기 때문에 가장 안정적인 방법. 하지만, 전체 데이터를 사용하면 그만큼 컴퓨터 자원을 많이 사용한다.

> #### 확률적 경사 하강법과 신경망 알고리즘
> 신경망은 일반적으로 많은 데이터를 사용하기 때문에 한 번에 모든 데이터를 사용하기 어렵다. 또 모델이 매우 복잡하기 때문에  
> 수학적인 방법으로 해답을 얻기 어렵다. 신경망 모델은 확률적 경사 하강법이나 미니배치 경사 하강법을 꼭 사용한다.

> #### 손실 함수와 비용 함수
> 비용 함수(cost function)는 손실 함수의 다른 말이다. 엄밀히 말하면 손실 함수는 샘플 하나에 대한 손실을 정의하고
> 비용 함수는 훈련 세트에 있는 모든 샘플에 대한 손실 함수의 합을 말한다. 

#### 로지스틱 손실 함수(이진 크로스엔트로피 손실함수)
<img width="90%" src="https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/73346564/644e3e1d-d9ad-44a0-a03b-20e4a511e766)https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/73346564/644e3e1d-d9ad-44a0-a03b-20e4a511e766"/>
   
양성 클래스(타깃=1)일때, 손실은 -log(예측 확률)로 계산. 확률이 1에서 멀어질수록 손실은 아주 큰 양수가 된다.       
음성 클래스(타깃=0)일때, 손실은 -log(1-예측 확률)로 계산. 확률이 0에서 멀어질수록 손실은 아주 큰 양수가 된다.      
> 타깃은 무조건 1로 바뀌어 곱해지므로 식을 간단하게 나타나기 위해 따로 사용하지 않는다
> 다중 분류에서 사용하는 손실함수를 크로스엔트로피 손실 함수 라고 부른다.

### SGDClassifier
- 다중 분류일 경우 SGDClassifier에 loss='log'로 지정하면 클래스마다 이진 분류 모델을 만든다.   
  이러한 방식을 OvR(One versus Rest)라고 한다.

> #### ConvergenceWarning 경고
> 모델이 충분히 수렴하지 않을경우 위의 경고 전송. 위의 경고를 보았다면, max_iter 매개변수의 값을 늘려주는 것이 좋다

### 에포크와 과대/과소적합
: 확률적 경사 하강법을 사용한 모델은 에포크 횟수에 따라 과소적합이나 과대적합이 일어날 수 있다    
에포크 횟수가 적으면 모델이 훈련 세트를 덜 학습한다. 마치 산을 다 내려오지 못하고 훈련을 마치는 셈이다.  
에포크 횟수가 충분히 많으면 훈련 세트를 완전히 학습하게 된다.

- 조기 종료 : 과대적합이 시작하기 전에 훈련을 멈추는
- loss 매개변수의 기본값은 < hinge > 이다.
- 힌지 손실(hinge loss)은 서포트 벡터 머신(support vector machine)이라 불리는 또 다른 머신러닝 알고리즘을 위한 손실 함수이다. 


# 질문
- #### StocaticGradientDescent(SGD) : 데이터 1개당 1 batch(random으로 뽑아서 중복이 될 수 있고, 수렴이 되지 않을 수 있다.)
#### Q. sigmoid 에서 x가 엄청 커져도 그 미분에 대해서 값의 차이가 크게 없다
#### A. 기울기의 차이가 없어서 미분값 손실 -> gradient vanishing 이라 한다.
