07-1 인공 신경망 
MNIST란? 
-> 머신러닝에서는 불꽃 데이터셋, 딥러닝에서는 MNIST 데이터셋이 유명 
MINIST 데이터는 손으로 쓴 0 ~ 9까지의 숫자로 이루어짐 
MINIST와 크기, 개수가 동일하지만 숫자 대신 패션 아이템으로 이루어진 데이터가 패션 MINIST 데이터셋

넘파이 배열의 nbytes 속성에 실제 해당 배열이 차지하는 바이트 용량이 저장되어 있음

인공신경망 / 생물학적 뉴런에서 영감을 받아 만든 머신러닝 알고리즘 
출력층 / 클래스 예측 신경망의 최종 값을 만든다는 의미 
뉴런(유닛) / 인공 신경망에서 z 값을 계산하는 단위
입력층 / 인공 신경망에서(x1 ~ x784) 

딥러닝은 인공 신경망과 거의 동의어로 사용되는 경우가 많음 심층 신경망을 딥러닝이라 부르기도 함 
심층 신경망 -> 여러 개의 층을 가진 인공 신경망

텐서플로 / 구글의 딥러닝 라이브러리
-> 케라스 / 텐서플로의 고수준 API 

케라스 라이브러리는 직접 GPU 연산을 수행하지 않음 대신 GPU 연산을 수행하는 다른 라이브러리를 백엔드로 사용

밀칩증 / 가장 간단한 인공 신경망의 층 
-> 양쪽의 뉴런이 모두 연결하고 있기 때문에 완전 연결층이라고도 부름 

활성화 함수 / 소프트맥스와 같이 뉴런의 선형 방적식 계산 결과에 적용되는 함수

원-핫 인코딩 / 정숫값을 배열에서 해당 정수 위치의 원소만 1이고 나머지는 모두 0으로 변환
-> 이런 변환이 필요한 이유는 다중 분류에서 출력층에서 만든 확률과 크로스 엔트로피 손실을 계산하기 위해 

Dense / 신경망에서 가장 기본 층인 밀집층을 만드는 클래스
Sequential / 케라스에서 신경망 모델을 만드는 클래스 
compile() / 모델 객체를 만든 후 훈련하기 전에 사용할 손실 함수와 측정 지표 등을 지정하는 메서드
evalute() / 모델 성능을 평가하는 메서드 

07-2 심층 신경망 

심층 신경망 / 2개 이상의 층을 포함한 신경망 
은닉층 / 입력층과 출력층 사이에 있는 모든 층 

회귀를 위한 신경망의 출력층에서는 어떤 활성화 함수를 사용하나요?
-> 분류 문제는 클래스에 대한 확률을 출력하기 위해 활성화 함수를 사용 

렐루 함수 / 이미지 분류 모델의 은닉층에서 많이 사용하는 활성화 함수 
-> 시그모이드 함수는 층이 많을수록 활성화 함수의 양쪽 끝에서 변화가 작기 때문에 학습이 어려워짐 
but 렐루 함수는 이런 문제가 없으며 계산도 간단

옵티마이저 / 신경망의 가중치와 절편을 학습하기 위한 알고리즘 or 방법 
-> 케라스에는 다양한 경사 하강법 알고리즘이 구현되어 있음 ex) SGD, 네스테로프 모멘텀, RMSprop, Adam 등

07-3 신경망 모델 훈련 

드롭아웃 / 은닉층에 있는 뉴련의 출력을 랜덤하게 꺼서 과대적합을 막는 기법
콜백 / 케라스 모델을 훈련하는 도중 어떤 작업을 수행할 수 있도록 도와주는 도구 
조기 종료 / 검증 점수가 더 이상 감소하지 않고 상승하여 과대적합이 일어나면 훈련을 계속 진행하지 않고 멈추는 기법
