<em> 기억보단 기록을 위해서 정리 후 작성하되, 블로그에 있는 교재에 종속적인 설명이나 세부적인 코드 이해 등 흔한 정리와는 다른 길을 가고 싶다는 생각으로 적었다. 물론 그게 결코 잘못된 것은 아니지만, 교재 상의 코드의 온전한 이해는 빙산의 일각이자 필요조건이니 그 내용을 여기 다 적는다고 좋은 게 아닐 거라 생각했다. 그래서 내 식대로, 본질적으로 '왜?' 라는 질문에 최대한 답할 수 있게끔 하면서 앞에서 다룬 것과 최대한 유기적으로 연결될 수 있게끔 정리하고 싶었다. 분리적 이해가 아닌, 포괄적인 흐름을 깨닫고 싶었다. 물론 이론보다 실습이 주가 돼야 함은 자명하나, 기반을 잘 다져서 후에 의문이 생겨도 금방 해소되지 않을까? 나름 예쁘게 정리하고 싶지만 고민으로 결정을 늦추는 게으른 팀원으로서 미안한 마음도 크긴 하다. </em>

인간의 사고 능력(학습, 추론 등)을 인공적으로 모방한 CS 분야인 AI는 ML과 DL으로 구분했었다.\
데이터로부터의 학습을 통해 예측 모델을 생성하는 ML에서 우리는 다양한 학습 알고리즘을 익혔다.\
그 다음은 DL인데, 인공 신경망(뉴런의 동작 원리 모방) 기반으로 층을 쌓아 올려 복잡한 문제 해결(비선형)까지 가능한 알고리즘이다.
 
![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/129747097/30e5058a-3817-4630-b9ef-f8af19c02933) 
단순히 아이디어만 따왔다고 생각하면 된다.
자극(입력)을 받아 내부에서 처리된 신경 전달 물질(활성화 함수)다음 신경세포(next layer 또는 output)로 넘겨주는 단순한 동작 원리이다.
추가로, 사진 하단의 설명을 잘 곱씹어 보자.

갑작스럽지만 복습 겸 연관지어 살펴보기 위해 초반부에 다룬 Linear Regression을 떠올려 보자.

작년에 선형대수학에서 '선형'이 단순 직선형을 의미하는 것이 아닌 선형결합의 성질을 따르는 본질적인 학문이라는 고민 해결의 추억 이후에도 
한 번 더 봉착한 문제가 바로 다중회귀, 다항회귀에서 '선형'이 무엇인지였고, 그렇게 다시 길을 잃게 되었다.
결론적으로는 관점의 차이라 생각한다. 사진으로 보자면
![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/129747097/8c44729f-24be-402b-ae06-72d9bf30c453)
피처에 대해서 보지 말고, 파라미터에 대해서 보자는 의미이다.

언뜻 보기에 일차함수만 직선 형태이니 다른 것들은 비선형 아닌가 싶겠지만, x,y 관계에서가 아닌 파라미터 입장에서 보았을 때 선형이라는 것이다.
(학창시절, 계수와 상수가 고정되었을 때 x값을 달리할 때 y값의 개형이 어떻게 되는지를 탐구했다면, 지금은 x,y가 고정된 상태(y는 레이블이니 지도학습)에서 파라미터가 달라지는 것을 파악하는 게 핵심이다.
물론 최적의 파라미터는 cost를 최소화하는 방향으로 탐색하다 구할 것이다.)

![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/129747097/e4a54a24-e8d4-4ba3-b2c5-0762d07a9812)
 
선형 회귀식(함수)을 신경망(이하 NN)으로 표현하면 그림처럼 정리된다.
앞에서 linear에 대해 계속 언급했는데, 중요한 건 cost로 최적의 파라미터를 찾는 것이다. 목적은 늘 그랬듯이 예측 잘 하는 모델 찾기니까.
역으로 2번 째 그림을 통해 NN으로 여러 함수를 표현할 수 있는데, input으로 피처/1, weight로 계수/상수를 부여하면 될 것이다.

Linear Regression을 NN으로 표현 가능하다면 Non-Linear Regression도 가능하지 않을까?
단순히 곱하고, 더해서 표현할 수 없는, 즉 파라미터에 대한 함수가 비선형적인 경우에도 신경망으로 표현 가능할지이다.  

현실의 고차원 혹은 비선형적 패턴의 데이터는 선형함수로의 모델링이 어렵다.
단순히 layer를 늘린다고, 즉 DNN으로 해결될까? 다시 말해, 후술할 활성화 함수가 단순한 예시로 y=x 꼴이라면 어떤 문제가 있을까?

만약 각 층에 대해 가중치를 연산한 결과를 변형 없이 그대로 다음 layer로 넘긴다면, 인공신경망은 여전히 선형 모델로 작동하게 된다. 
각 층의 출력은 그 층의 가중치와 입력의 선형 결합에 해당하는 것으로, 이러한 선형 변환은 각 층을 통과할 때 계속 적용될 것이다.
따라서 모든 층에 선형 함수를 사용한다면, 신경망은 여전히 단일 선형 함수로 모델링될 것이며, 
이는 결과적으로 여러 층을 쌓더라도 여전히 단일한 선형 변환으로 나타낼 수 있으므로, 모델의 표현력이 크게 향상되지 않을 것이다.
합성함수처럼 생각한다면, 거듭된 합성으로 값을 변형하려는데 f(x) = x에 대해 f(f(f(...(c))...) = c로 변형의 의미가 없는 것이다.
이것이 활성화 함수의 필요성이다. 

세부적인 활성화 함수의 종류(sigmoid, softmax, ReLU의 세부적 특징과 한계는 당장은 다루지 않겠다.)
앞에서 편의상 linear에 대해 다룬 후 non-linear로 넘어갔는데, 이번에는 로지스틱 회귀를 떠올려 보자.
로지스틱 회귀는 0~1 사이의 확률값을 도출하여 사용하는 알고리즘으로 대상은 선형 모델이었다. (z에 대한 선형방정식 연상)
로지스틱 회귀는 간단한 신경망과 형태가 흡사하다. 

![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/129747097/00bc3f78-1288-44f6-b367-e36895342f39)

물론 머신러닝에서와 딥러닝에서의 활성화 함수는 수식 상의 차이는 없으나 세부적인 용도 차이를 알아두자.
머신러닝, 특히 로지스틱 회귀의 경우 '분류'를 위한 '확률값' 출력이었고,\
딥러닝, 인공 신경망의 경우 다음 layer로 통과할 수 있도록 변형하기 위함이었다.



