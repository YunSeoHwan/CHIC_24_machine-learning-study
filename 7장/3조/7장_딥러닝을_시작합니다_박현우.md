### <여섯 번째 스터디 - 2024.2.8.>
### 범위: p.340~420

### 딥러닝과 XAI(Explainable AI)
- XAI(Explainable AI) : 해석력을 가진 AI
- 딥러닝: 블랙박스 모델임. 어떤 변수가 설명력을 가지는지, 어떤 계산과정을 거치는지 알 수 없음
- 딥러닝 모델을 통한 XAI는 할 수 있으면 좋지만 어려움이 많음. -> SHAP, LIME이 XAI를 위한 기법.
- 이미지 관련 비전 관련 TASK -> 비교적 XAI를 구현할 수 있는 가능성이 높아짐 (말이라는 것을 어디를 보고 분류했는지 등등) 그러나, 나머지 분야에서는 불가능할 수 있다.
- 이미지를 분류할 때 딥러닝(CNN)을 사용하기 때문에 CNN 모델의 메커니즘을 알아보는 방식으로 접근하는 것이 좋을 것으로 보임.
### 딥러닝에서 나오는 최적의 하이퍼파라미터 값을 찾는 방법
- optuma를 사용하면 자동으로..
- 사람이 찾기 어려움 (딥러닝 모델은 할 때마다 결과가 다름) 예를들어, 베타를 1로 설정했더니 accuracy가 얼마, 베타를 바꾸면 또 안 나올 수도 있음
- 수식적으로 접근하면 왜 어려운지 알 수 있음
- epoch를 늘리면 성능이 좋아지지만 과하면 안된다.
- 실험적으로 찾아야함
### 비선형적인 Activation Fuction을 사용하는 이유
-  선형적인 활성화 함수를 사용하면 layer를 많이 사용할 이유가 없음. 따라서 선형적인 활성화 함수를 사용하지 않음.
### dense층의 Activation Fuction
- 활성화 함수는 하나로 가는 것이 일반적임. 일반적으로는 relu를 많이 사용함.
### 출력층의 Activation Fuction(softmax함수)
- softmax를 사용하는 이유는 다중분류를 하기 위한 것 (dense층의 Activation Function과 다른 의미로 사용)
### 책에서 parameter의 개수를 세는 부분
- 뉴런층을 보면 노드로만 이루어져 있는데, 다음 레이어로 넘어갈 때 bias term(상수)가 존재함. (책에서는 bias term이 생략되었음)
### p.343에서의 labeling vs 1장에서 다중분류에 대한 labeling
- 1장에서 encoding 하는 과정에서 그냥 0,1,2로 labeling을 하면 데이터의 특성이 망가질 수 있다.
-  현재 페이지에서는 최종적으로 예측하는 값이므로 상관없음(종속변수 Y(결과값)이기 때문)
- feature(x내의 하나의 특성) 이면 0,1,2임을 고려해야 하지만, 7장에서의 0~9는 상관이 없음 (티셔츠와 바지의 관계는 관심이 없음)
### 선형적인 Activation Fucntion을 사용하지 않는 이유
- 선형적으로 표현한다고 해서, 복잡한 데이터를 표현하지 못하는 것은 아님
- 그러나, real world에서는 비선형적인 데이터가 많음
- 예를 들어, 불량품 분류 문제에서 불량품은 다양한 변인의 결과. 하나의  feature의 영향이 아니고, 더 다양한 문제를 해결할 수 있으므로 비선형적 활성화함수를 사용함.
- 선형적으로 사용하면 한번의 layer를 쌓은 것과 같아짐 -> 확률적으로, 다양한 문제를 사용하지 못할 가능성이 생김. 물론 선형적인 활성화 함수의 사용도 가능은 하고, 선형적으로도 꽤 복잡한 것을 해결할 수 있음.
### 딥러닝에서는 언더피팅 문제는 현저히 없는가?
- 일반적으로는 그러함. layer를 깊게 쌓다보면 일반적으로 많은 요소를 고려하므로, 언더피팅이 잘 발생하지는 않음.(없는 것은 아님)
- 그리고 성능이 너무 낮으면 (자연스레) 사용하지 않을 것.
### 다양한 Optimizer
- 스텝'방향' -> 방향을 고려하는 이유. -> 사진(일반적인 SGD에서 optimal point를 찾아가는 과정) vs (Momentum을 사용하였을 때 step의 방향: 가는 방향을 고려)
속력과 속도에서 속도를 고려한다.
- momentum 수식(사진)
- momentum이 더 안정적임. 방향: local optimum에 빠지는 것을 어느 정도 방지
- '스텝사이즈' Ad : Adaptied 
- 변화에 대한 가중치를 줌. 기울기 값이 크면 좀 더 커지고 기울기 값이 작아지면 좀 더 작아지는..
- 기울기에 대한 업데이트 (학습률,learning rate)를 조절하는 것.
- RMSProp : 어느 지점에서 얼마나 떨어졌냐도 고려하는 것.
- Adam은 둘 다 사용하는 것. 그러다 보니 hyperparameter 값이 더 많음(설정해 줘야 하는 값이 더 많음)
![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/128834657/1e020380-540d-46a4-8b10-fd0f391e1247)
### 가중치에 대한 계산
-![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/128834657/ed595b99-4969-4f31-9cc1-3bd18cc7d07c)

