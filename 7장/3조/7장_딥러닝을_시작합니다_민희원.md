# 7장 딥러닝을 시작합니다
p.339 ~

## 인공신경망
### TensorFlow와 Keras 비교
Tensorflow: 구글에서 개발하고 오픈소스로 공개한 머신러닝 라이브러리

Keras: Tensorflow위에서 동작하는 라이브러리

### 텐서플로
데이터 플로우 그래프 (Data Flow Graph) 구조를 사용하는 프레임워크. 각종 신경망 학습에 사용되며, 실제로 이미지 인식, 반복 신경망 구성, 기계 번역, 필기 숫자 판별 등에 활용되고 있음.

장점
- 데이터 플로우 그래프를 통한 풍부한 표현이 가능함
- 계산 구조와 목표 함수만 정의하면 자동으로 미분 계산을 처리함
- 텐서보드를 통해서 파라미터 변화 양상 및 DNN 구조를 알 수 있음

단점
- 메모리를 효율적으로 사용하지 못하고 있음
- Symbolic Loop 기능이 유연하지 못하며, 함수가 있어도 텐서 타입으로만 적용해야 함

### 케라스
구글에서 프로젝트 연구적 노력의 목적으로 개발된 프레임워크. 파이썬으로 작성된 오픈 소스 신경망 라이브러리임. 딥 신경망과의 빠른 실험을 가능케 하도록 만들어졌으며 최소한의 모듈 방식의 확장 가능성에 초점을 둠.

장점
- 사용자 친화성, 모듈성, 확장성이있어서 일관되고 간결한 API를 제공함
- 배우기 쉽고 모델 구축이 쉬움
- 컨볼루션 신경망, 순환 신경망, -그리고 둘의 조합까지 모두 지원

단점
- 어떤 오류가 발생했을 때 케라스 자체의 문제인지, 백엔드 언어의 문제인지 특정하기 어려움
- 문서화가 제대로 되어 있지 않고 이용자 수가 적어 참고할 곳이 부족함

### 인공 뉴런
**생물학적 뉴런과 로지스틱 회귀 모델**
- 생물학적 뉴런의 역할은 앞 뉴런들에서 받은 정보를 처리해서, 종합해서 처리한 후 활성화를 통해 뒤에 있는 뉴런들에게 정보를 전달
- 머신 러닝 모델을 인공 뉴런으로 사용
- 로지스틱 회귀 모델도 입력 데이터를 받아서 종합, 처리한 후, 시그모이드 함수를 통해 0~1 사이 값으로 리턴

### 인공 신경망
입력층 -> 은닉층 -> 출력층

은닉층: 뉴런의 가중치는 입력패턴에 숨겨져있는 특성을 나타내며 출력층을 통한 출력 패턴의 계산에 이 특성이 반영된다.

수 많은 인공 뉴런이 엮여있는 모델. 원 하나는 인공 뉴런 하나를 의미하고, 선은 각 가중치를 의미함.(편향은 뉴런 안에 있음.)

#### 학습
- 인공 신경망의 목적은 주어진 데이터에 잘 맞는 가중치와 편향들을 찾아내는 것
- 잘 맞는 가중치와 편향을 찾는 걸 신경망을 학습한다라고 표현

#### 예측
- 신경망 입력층에 데이터를 넣어서 미자믹 층 뉴런까지 계산하면 가장 출력이 높은, 가장 많이 활성화되는 뉴런의 결과값을 리턴

#### Parameter 수 계산
1. Fully Connected Layer
   - 두 layer 사이를 계산하는 것
   - ('Previous layer length'+1) * 'Current layer length'
2. Convolutional layer
   - 이미지 학습할때 자주 사용되는 방식
   - (('Current filter length * Current filter width')*Previous filter size +1)* Current filter size
   - 현재 layer 의 크기를 이전 부피에 곱하고 +1 해주고 현재 부피에 곱하면 됨.

## 심층 신경망
심층 신경망과 일반적인 (단일층) 신경망의 핵심적인 차이점는 층의 개수(깊이)임.

전통적인 기계학습 알고리즘은 하나의 입력과 하나의 출력 층으로 이루어져 있으며 많아야 중간에 하나의 은닉층을 가지고 있음. 이런 구조의 신경망을 얕다(shallow)고 표현함. 입력과 출력 층을 포함해 3개가 넘는 층을 (즉 2개 이상의 은닉층을) 갖는 경우에 우리는 이 신경망이 깊다(deep)고 표현함.

### ReLU 함수
왜 시그모이드가 아닌 렐루를 사용할까?

시그모이드는 S자 곡선을 가지고, 0과 1 사이의 출력 범위를 가진다. 즉 x가 음의 무한대에 가까워지면 출력도 0에 가까워지고, 양의 무한대에 가까워지면 출력은 1에 가까워진다.
 

반면 ReLU는 양수면 입력값을 출력하고, 아니면 0을 출력한다. 양수의 입력값에 대해서는 기울기가 1이고, 음수 입력 값에 대해서는 기울기가 0이다.

#### ReLU가 Sigmoid보다 선호되는 이유
1. 계산 효율성
   - 활성화 함수의 계산 효율성은 입력이 주어졌을 때 출력을 얼마나 빨리 계산할 수 있느냐임.
   - 대규모 데이터셋이나 복잡한 모델을 다룰 때는 당연히 더 간단한 함수가 계산 효율성이 높을 것임.
   - sigmoid는 지수 함수가 포함되기 때문에 나눗셈 같은 복잡한 수학적 연산이 필요하지만, ReLU는 간단하계 계산할 수 있기 때문에 계산 효율성을 높일 수 있고 이는 훈련 프로세스를 더 빠르게 만들고 최적의 값으로 수렴하는 속도도 높여줄 수 있음
2. Gradient(기울기) 소실 방지
   - sigmoid 함수는 그래디언트 소실 문제가 있음. 입력값이 매우 크거나 작다면 기울기도 0에 가까워짐.
   - 기울기가 0에 가까워지면 가중치가 매우 느리게 업데이트되므로 학습 과정이 늦어지고, 신경망이 수렴하기 어렵게 된다. local minimum에 갇히게 될 수도 있음. ReLu는 **양수 입력 값에 대해 일정한 기울기를 갖고 있으므로 이 문제를 방지**할 수 있음.
3. Saturation problem(포화 문제) 방지 : 심층 네트워크에 더 적합함
   - 시그모이드에서 발생하는 포화 문제를 방지할 수 있음. 포화는 활성화 함수의 출력이 1 또는 0에 매우 가까워져서 가중치와 편향을 업데이트하기 어려워져버려 신경망의 학습 능력이 제한되어 버리는 경우임.
4. ReLU는 sparse activation(희소 활성화)를 생성함
   - 희소 활성화라는 의미는 많은 뉴런이 0을 출력하게 해서 활성화를 적게 한다는 뜻.
   - ReLU함수의 그래프를 생각해보면, 음수는 모두 0을 출력하고 양수만 변경하지 않고 출력하게 되는데 이를 희소 활성화라고 함. 이렇게 하면 계산 비용이 줄고, 과적합을 줄이는 데도 도움이 됨.

ReLU 함수에도 단점이 존재하고, 상황에 따라 시그모이드 사용이 더 적절한 때도 있음.

#### 활성화 함수를 비선형 함수로 써야하는 이유 
신경망에서 선형 함수를 쓰면 층을 아무리 깊게 해도 은닉층이 없는 네트워크와 똑같아져버려서 신경망의 장점을 살릴 수 없게 됨. 즉, 층을 쌓기 위해서 비선형 함수를 사용하는 것.