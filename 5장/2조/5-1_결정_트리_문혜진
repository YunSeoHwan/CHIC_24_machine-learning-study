# 결정 트리
: 예/아니오에 대한 질문을 이어나가면서 정답을 학습하는 알고리즘
- 비교적 예측 과정을 이해하기 쉽고 성능도 띄어남 → 비교적 비전문가에게도 설명하기 쉬운 모델을 만듦
- 리프 노드에서 가장 많은 클래스가 예측 클래스가 됨(k-최근접 이웃과 매우 비슷)
- 결정 트리는 표준화 전처리 과정 필요 없음
  - 이유 : 불순도를 기준으로 샘플을 나눔, 불순도는 클래스별 비율을 가지고 계산, 샘플을 어떤 클래스 비율로 나누는지 계산할 때
           특성값의 스케일이 계산에 영향을 미치지 않음 즉 특성값의 스케일은 결정 트리 알고리즘에 아무런 영향을 미치지 않음
  - 특성값을 표준점수로 바꾸지 않으면 좀 더 이해하기 쉬움
-  많은 앙상블 학습 알고리즘의 기반이 됨
- DecisionTreeClassifier 클래스
  - 사이킷런이 제공하는 결정 트리 알고리즘
  - fit() 메서드를 호출해 모델 훈련한 다음 score() 메서드로 정확도 평가
- 결정 트리는 위에서부터 아래로 거꾸로 자람
  - 가지(branch)는 테스트의 결과(True, False)를 나타내며 일반적으로 하나의 노드는 2개의 가지 가짐
  - 노드 : 결정 트리를 구성하는 핵심 요소, 훈련 데이터의 특성에 대한 테스트를 표현
    - 루트 노드 : 맨 위의 노드
    - 리프 노드 : 맨 아래 끝에 달린 노드
- plot_tree() : 사이킷런에서 제공, 이 함수를 사용해 결정 트리를 이해하기 쉬운 트리 그림으로 출력해줌
  - max_depth : 트리 깊이 결정 ex) max_depth = 1 이면, 루트 노드를 제외하고 하나의 노드를 더 확장해 그림
  - filled : 클래스에 맞게 노드의 색을 칠할 수 있음, 
    - filled = True 로 지정시 클래스마다 색깔 부여하고 어떤 클래스의 비율이 높아지면 점점 진한 색으로 표시
  - feature_names : 특성의 이름을 전달할 수 있음

# 불순도
: 결정 트리가 최적의 질문을 찾기 위한 기준
- 사이킷런은 지니 불순도와 엔트로피 불순도를 제공함
- gini : 지니 불순도를 의미, DecisionTreeClassifier 클래스의 criterion 매개변수의 기본값
  - 계산 : 클래스의 비율을 제곱해서 더한 다음 1에서 빼면 됨
          → 지니 불순도 = 1 - (음성 클래스 비율^2 + 양성 클래스 비율^2)
  - 다중 클래스 문제일 경우 클래스가 더 많겠지만 계산하는 방법의 동일
  - 어떤 노드의 두 클래스의 비율이 정확히 1/2씩이라면 지니 불순도는 0.5가 되어 최악이 됨
  - 노드에 하나의 클래스만 있다면 지니 불순도는 0이 되어 가장 작음, 이런 노드를 순수 노드라 부름
- DecisionTreeClasifier 클래스에서 criterion='entropy) 지정시 엔트로피 불순도 사용 가능
  - 엔트로피 불순도 : 노드의 클래스 비율을 사용하지만 지니 불순도처럼 제곱이 아니라 밑이 2인 로그 사용해 곱함
    - 계산 : - 음성 클래스 비율 * log2(음성 클래스 비율) - 양성 클래스 비율 * log2(양성 클래스 비율)
*criterion 매개변수의 용도는 노드에서 데이터를 분할할 기준을 정하는 것

# 정보 이득
: 부모 노드와 자식 노드의 불순도 차이
- 결정 트리 모델은 부모 노드와 자식 노드의 불순도 차이가 가능한 크도록 트리 성장 시킴
- 결정 트리 알고리즘은 정보 이득이 최대화되도록 학습함
- 계산 : 자식 노드의 불순도를 샘플 개수에 비례해 모두 더함, 그다음 부도 노드의 불순도에서 빼면 됨
  → 부모의 불순도 - (왼족 노드 샘플 수/ 부모의 샘플수) * 왼쪽 노드 불순도 - (오른쪽 노드 샘플 수/ 부모의 샘플 수)* 오른쪽 노드 불순도

==> 불순도 기준을 사용해 정보 이득이 최대가 되도록 노드 분할, 노드를 순수하게 나눌수록 정보 이득 커짐
==> 새로운 샘플 예측시 노드의 질문에 따라 트리 이동, 마지막에 도달한 노드의 클래스 비율을 보고 예측을 만듦

# 가지치기
: 결정 트리의 성장을 제한하는 방법
- 결정 트리는 제한 없이 성장하면 훈련 세트에 과대적합되기 쉬움
- 사이킷런의 결정 트리 알고리즘은 여러개의 가지치기 매개변수 제공
- 결정 트리에서 가지치기를 하는 가장 간단한 방법은 자라날 수 잇는 트리의 최대 깊이를 지정하는 것

# 특성 중요도
: 결정 트리에 사용된 특성이 불순도를 감소하는데 기여한 정도를 나타내는 값
- 결정 트리는 어떤 특성이 가장 유용한지 나타내는 특성 중요도를 계산 해 줌
  → 특성 중요도를 활용하면 결정 트리 모델의 특성 선택에 활용 가능
- 특성 중요도를 계산할 수 있는 것이 결정 트리의 또 다른 큰 장점
- feature_importaces_ 속성 : 특성 중요도는 결정 트리 모델의 이 속성에 저장되어 있음
  - 출력 값 모두 더하면 1 됨
  - 특성 중요도는 각 노드의 정보 이득과 전체 샘플에 대한 비율을 곱한 후 특성별로 더해 계산함
