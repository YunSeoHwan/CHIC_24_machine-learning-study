함수명.info() : 각 column의 데이터 타입과 누락된 데이터 있는지 출력  
함수명.describe() : 각 column의 간단한 통계(최소, 최대, 평균값 등) 출력 => 각 열의 스케일이 같은지 다른지 판단할 때 쓰일 수 있음  
<br />
# 결정트리
DecisionTreeClassifier 사용해서 만듦
```
#DecisionTreeClassifier 사용해서 결정트리 만들기
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(train_scaled, train_target)
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))
#과대적합

#길이를 제한해서 출력
plt.figure(figsize=(10,7))
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```
결정트리 읽는 법
- 박스 안 정보 : [테스트조건 / **불순도** / 총샘플수 / 클래스별샘플수]  
ex) 5197개 중에 sugar -0.239이하가 1258개, 이상이 3939개 => 클래스 참일 확률 0.367 => 오른족이 더 짙은 파란색
- 왼쪽 화살표 : 조건 만족
- 오른쪽 화살표 : 조건 불만족
- 클랙스의 비율이 높아지면 점점 짙은 색으로 표시
<br />
**지니 불순도** : 1 - (음성클래스비율^2 + 양성클래스비율^2)  
0.5가 최악, 0은 순수 노드  
부모와 자식 노드 사이의 불순도 차이 : 정보이득 => 정보이득이 최대가 되도록 데이터를 나눌 수 있음  
<br />
**엔트로피 불순도** : -음성클래스비율*log2(음성클래스비율) -양성클래스비율*log2(양성클래스비율)  
=> 지니 불순와 달리 밑이2인 로그를 사용함  
<br />
두 값의 차이는 크지 않음  

### 가지치기
과대적합을 막기 위해서는 가지치기를 해야함  
자라날 수 있는 트리의 최대 깊이를 지정하며 가지치기를 하는 방법이 가장 간단함




