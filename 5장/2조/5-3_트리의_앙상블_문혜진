# 정형 데이터, 비정형 데이터
- 정형 데이터 : 어떤 구조로 되어 있다는 뜻 → csv, 데이터베이스 혹은 엑셀에 저장하기 쉬움
  ex) 온라인 쇼핑몰에 진열된 상품, 구매한 쇼핑 정보 등
- 비정형 데이터 : 정형 데이터의 반대, 데이터베이스나 엑세로 표현하기 어려운 것들
  ex) 사진, 책의 글과 같은 텍스트 데이터, 핸드폰으로 듣는 디지털 음악 등

# 앙상블 학습
: 정형데이터를 다루는 데 가장 뛰어나 성과를 내는 알고리즘
- 더 좋은 예측 결과를 만들기 위해 여러 개의 모델을 훈련하는 머신러닝 알고리즘
- 대부분 결정 트리를 기반으로 만들어져 있음
* 비정형 데이터 -> 신경망 알고리즘 사용

# 랜덤 포레스트
: 결정 트리를 랜덤하게 만들어 결정 트리(나무)의 숲을 만든 후 각 결정 트리의 예측을 사용해 최종 예측을 만듦
- 대표적인 결정 트리 기반의 앙상블 학습 방법
- 부트스트랩 샘플을 사용하고 랜덤하게 일부 특성을 선택해 트리를 만드는 것이 특징
  - 부트스트랩 :데이터 세트에서 중복을 허용해 데이터를 샘플링하는 방식
    ex) 100개의 샘플이 있을 때 1개를 뽑고 다시 가방에 넣어 그 다음 샘플을 뽑는 방식
- 각 노드를 분할할 때 전체 특성 중에서 일부 특성을 무작위로 고른 다음 이 중에서 최선의 분할을 찾음
- 랜덤하게 선택한 샘플과 특성 사용 → 훈련 세트에 과대적합되는 것을 막아주고 검증 세트와 테스트 세트에서 안정적인 성능 얻음
- 분류 모델인 RandomForestClassifier : 기본적으로 전체 특성 개수의 제곱근만큼의 특성 선택 
  ex) 4개 특성 존재, 노드마다 2개 랜덤하게 선택해 사용
  - 자체적으로 모델을 평가하는 점수를 얻을 수 있음
    - OOB 샘플 : 부트스트랩 샘플에 포함되지 않고 남는 샘플
    - OOB 샘플을 사용해 부트스트랩 샘플로 훈련한 결정 트리를 평가할 수 있음 → 검증 세트의 역할과 비슷
    - oob_score  매개변수를 True로 지정해야 점수 얻을 수 있음(기본 값 False)
    - OOB 점수 사용시 교차 검증을 대신할 수 있어 결과적으로 훈련 세트에 더 많은 샘플을 사용할 수 있음
  - 회귀 모델인 RandomForestRegressor는 전체 특성 사용
- 랜덤 포레스트는 결정 트리의 앙상블이기 때문에 DecisionTreeClassifier가 제공하는 중요한 매개 변수 모두 제공함
  +) 결정 트리의 가장 큰 장점 중 하나인 특성 중요도도 계산 가능
  - 랜덤 포레스트의 특성 중요도는 각 결정 트리의 특성 중요도를 취합한 것

# 엑스트라 트리
- 랜덤 포레슽와 비슷하게 결정 트리를 사용해 앙상블 모델을 만듦
- 랜덤 포레스트와 동일하게 결정 트리가 제공하는 대부분의 매개변수 지원
- 전체 특성 중에 일부 특성을 랜덤하게 선택해 노드를 분할하는데 사용
- 랜덤 포레스트와의 차임점 : 부트스트랩 샘플 사용 안 함
  - 결정 트리를 만들 때 전체 훈련 세트를 사용, 대신 노드를 분할할 때 가장 좋은 분할을 찾는 것이 아닌 무작위로 분할 함
  - 엑스트라 트리가 사용하는 결정트리 : splitter = 'random'인 결정 트리
  - 하나의 결정 트리에서 특성을 무작위로 분할한다면 성능이 낮아지겠지만 많은 트리를 앙상블하기 때문에 
    과대 적합을 막고 검증 세트의 점수를 높이는 효과 있음
- ExtraTreesClassifier : 사이킷런에서 제공하는 엑스트라 트리
  - 보통 엑스트라 트리가 무작위성이 좀 더 크기 때문에 랜덤 포레스트보다 더 많은 결정 트리 훈련
  - but 랜덤하게 노드를 분할해 빠른 계산 속도가 장점임!
  - 랜덤포레스트와 마찬가지로 특성 중요도 제공
  - 엑스트라 트리의 회귀 버전은 ExtraTreesRegressor 클래스

# 그래이디언트 부스팅
: 깊이가 얕은 결정 트리를 사용해 이전 트리의 오차를 보완하는 방식으로 앙상블하는 방법 
- 랜덤 포레스트나 엑스트라 트리와 다름
- 그래이디언트 부스팅도특성 중요도 제공함 
- 깊이가 얕은 결정 트리 사용 → 과대적합에 강하고 일반적으로 높은 일반화 성능 기대 가능
- 경사 하강법을 사용해 트리를 앙상블에 추가
  - 분류 : 로지스틱 손실 함수 사용
  - 회귀 : 평균 제곱 오차 함수사용
- 결정 트리를 계속 추가하면서 가장 낮은 곳을 찾아 이동
  - 결정 트리를 연속적으로 추가해 손실 함수를 최소화하는 앙상블 방법
- GradientBoostingClassifier : 사이킷런 제공
  - 기본적으로 깊이가 3인 결정 트리를 100개 사용
  - subsample : 트리 훈련에 사용할 훈련 세트의 비율을 정함
    - 기본값 = 1.0 → 전체 훈련 세트 사용
    - 값이 1보다 작으면 훈련 세트의 일부 사용
    ==> 경사 하강법에서의 확률적 경사 하강법이나 미니배치 경사 하강법과 비슷
  - n_jobs 매개변수 X
- GradientBoostingRegressor : 그레이디언트 부스팅의 회귀 버전
- 훈련 속도가 조금 느리지만 더 좋은 성는 기대 가능

# 히스토그램 기반 그레이디언트 부스팅
- 그래이디언트 부스팅의 속도를 개선한 것
- 안정적인 결과와 높은 성능으로 인기 많음
- 정형 데이터를 다루는 머신러닝 알고리즘에서 가장 인기가 높은 알고리즘
- 입력 특성을 256개의 구간으로 나눔 → 노드를 분할할 때 최적의 분할을 매우 빠르게 찾을 수 있음
- 256개의 구간 중에서 하나를 떼어 놓고 누락된 값을 위해서 사용 → 입력에 누락도니 특성이 있더라도 이를 따로 전처리할 필요 X
- HistGradientBoostingClassifier : 사이킷런의 히스토그램 기반 그레이디언트 부스팅 클래스
  - 기본 매개변수에서 안정적인 성능을 얻을 수 있음
  - max_iter : 부스팅 반복 횟수 지정 ← 트리 개수 지정하는데 n_estimatos 대신 사용
    - 성능을 높이고 싶으면 이 매개변수를 테스트하면 됨
  - permutation_importance() 함수 : 트그성 중요도를 계산하기 위한 함수
    - 특성을 랜덤하게 섞어 모델의 성능이 변화하는지를 관찰해 어떤 특성이 중요한지 계산
    - 훈련 세트뿐만 아니라 테스트 세트에도 적용 가능, 사이킷런에서 제공하는 추정기 모델에 모두 사용 가능
    - n_repeats : 랜덤하게 섞을 횟수 지정
    - 이 함수가 반환하는 객체는 반복해 얻은 특성 중요도, 평균, 표준 편차 담고 있음
- HistGradientBoostingRegressor 클래스 : 히스토그램 기반 그레이디언트 부스팅의 회귀 버전

* XGBoost :사이킷런 외에 히스토그램 기반 그레이디언트 부스팅 알고리즘 구현한 라이브러리 여럿 중 하나
  - 코랩에서도 사용 가능, 사이킷런의 cross_validate() 함수와 함께 사용 가능
  - 다양한 부스팅 알고리즘 지원
  - tree_method 매개변수에 'hist' 지정시 히스토그램 기반 그레이디언트 부스팅 사용 가능
* LightGBM : 마이크로소프트에서 만든 히스토그램 기반 그레이디언트 부스팅 라이브러리
  - 코랩에 이미 설치 되어 잇음 → 바로 사용 가능
