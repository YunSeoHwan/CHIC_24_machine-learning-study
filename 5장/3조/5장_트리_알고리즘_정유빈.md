# 용어
- 사분위수 : 데이터를 순서대로 4등분 한 값   
   예를 들어, 2사분위수(중간값)는 데이터를 일렬로 늘어놓았을 때 정중앙의 값. 만약 데이터의 개수가 짝수라 중앙값을 선택할 수 없다면 가운데 2개의 평균값을 사용한다
- StandardScaler 클래스 : 모든 feature들을 평균:0, 분산:1 로 조정(표준화 하기)
- 불순도 : 결정 트리가 최적의 질문을 찾기 위한 기준
- 정보 이득 : 부모 노드와 자식 노드의 불순도 차이(결정 트리 알고리즘은 정보 이득이 최대화되도록 학습)
- 특성 중요도 : 결정 트리에 사용된 특성이 불순도를 감소하는데 기여한 정도를 나타내는 값
- 검증 세트 : 하이퍼파라미터 튜닝을 위해 모델을 평가할 때, 테스트 세트를 사용하지 않기 위해 훈련 세트에서 다시 떼어 낸 데이터 세트
- 교차 검증 : 훈련 세트를 여러 폴드로 나눈 다음 한 폴드가 검증 세트의 역할을 하고 나머지 폴드에서는 모델을 훈련 한다.   
       이런식으로 모든 폴드에 대해 검증 점수를 얻어 평균하는 방법
- 그리드 서치 : 하이퍼파라미터 탐색을 자동화해 주는 도구. 탐색할 매개변수를 나열하면 교차 거즘을 수행하여 가장 좋은 검증 점수의 매개변수 조합을 선택
- 랜덤 서치 : 연속된 매개변수 값을 탐색할때 유용. 탐색할 값을 직접 나열하는 것이 아닌 탐색 값을 샘플링할 수 있는 확률 분포 객체를 전달 
- 앙상블 학습 : 더 좋은 예측 결과를 만들기 위해 여러 개의 모델을 훈련하는 머신러닝 알고리즘  
- 랜덤 포레스트 : 부트스트랩 샘플을 사용하고 랜덤하게 일부 특성을 선택하여 트리를 만드는 것이 특징
- 엑스트라 트리 : 부트스트랩을 사용하지 않고 대신 랜덤하게 노드를 분할해 과대적합을 감소시킨다
- 그레이디언트 부스팅 : 랜덤 포레스트와 엑스트라 트리와 달리 결정 트리를 연속적으로 추가하여 손실 함수를 최소화 하는 앙상블 방법
      훈련 속도가 조금 느리지만, 좋은 성능을 기대할 수 있다.
- 히스토그램 기반 그레이디언트 부스팅 : 그레이디언트 부스팅 속도를 개선한 것이다.

# 5-1 결정 트리
: 결정 트리는 특정 기준(질문)을 따라 데이터를 구분 짓는다. 결정 트리의 가장 첫 번째 기준은 트리에서 최초 깊이인 루트 노드(Root node)에서 시작하며, 트리가 하위 깊이로 깊어지면서 생성되는 새로운 분류 기준이 되는 질문을 노드(Node)라고 말하며, 더 이상 분기가 되지 않는 마지막 노드를 'Terminal' 또는 'Leaf Node' 라고 한다.

- #### 데이터에 누락된 값이 있다면?  
  : 그 데이터를 버리거나 평균값으로 채운 후 사용할 수 있다.

- 어떤 클래스의 비율이 높아지면 점점 진한 색으로 표시한다
- #### 결정 트리에서 예측하는 방법
   : 리프 노드에서 가장 많은 클래스가 예측 클래스가 된다.

- #### 결정 트리의 장점
   : 표준화 전처리 과정이 필요가 없다.    
     특성 중요도를 계산해준다  
  (특성 중요도는 각 노드의 정보 이득과 전체 샘플에 대한 비율을 곱한 후 특성별로 더하여 계산)

> 결정 트리를 회귀 문제에 적용하면 리프 노드에 도달한 샘플의 타깃을 평균하여 예측값으로 사용

### 불순도
: gini는 지니 불순도(Gini inpurity)를 의미한다.

지니 불순도 = 1 - (음성 클래스 비율^2 + 양성 클래스 비율^2)  
> ex. 10개의 구슬 중 파란구슬 3개   
> 1-((3/10)^2 + (7/10)^2) = 0.42    
> 42% 정도의 불순도를 구한다... 잘 분류되었으면 0 에 가까워진다  

- 어떤 노드의 두 클래스 비율이 정확히 1/2 씩이라면 지니 불순도는 0.5가 되어 최악의 상황이 된다  
- 노드에 하나의 클래스만 있다면 지니 불순도는 0이 되어 가장 작습니다. 이러한 노드를 순수 노드라고 부른다
- 노드를 순수하게 나눌수록 정보이득이 커진다

- 결정 트리 모델은 부모 노드와 자식 노드의 불순도 차이가 가능한 크도록 트리를 성장 시킨다.
> 부모의 불순도 - (왼쪽 노드 샘플 수 / 부모의 샘플 수) x 왼쪽 노드 불순도 - (오른쪽 노드 샘플 수 / 부모의 샘플 수) x 오른쪽 노드 불순도

#### 이러한 부모와 자식 노드 사이의 불순도 차이를 정보 이득이라고 부른다. 

- DecisionTreeClassifier 클래스에서 criterion = 'entropy'를 지정하여 엔트로피 불순도를 사용할 수 있다
- 엔트로피 불순도도 노드의 클래스 비율을 사용하지만, 지니 불순도처럼 제곱이 아닌 밑이 2인 로그를 사용하여 곱한다.
> 음성 클래스 비율 x log(음성 클래스 비율) - 양성 클래스 비율 x log(양성 클래스 비율)

### 가지치기
: 가지치기를 하는 간단한 방법은 트리의 최대 깊이를 지정하는 것이다.(맨 위의 부모노드는 깊이 : 0)

# 5-2 교차 검증과 그리드 서치

> ### 문제 발생 : 테스트 세트를 사용해 자꾸 성능을 확인하다 보면 점점 테스트 세트에 맞추게 된다

### 검증 세트
: 훈련 세트를 또 나누어서 검증 세트를 만들기
이전) 20% : 테스트 , 80% : 훈련
80%의 훈련 세트 중에서 20%를 떼어 내어 검증 세트로 만들기
- 60 : 20 : 20 = 훈련 : 검증 : 테스트
> #### 테스트 세트와 검증 세트에 얼마나 많은 샘플을 덜어놔야 하는지?
> 보통 20~30%를 테스트 세트와 검증 세트로 떼어 놓는다. 하지만, 훈련 데이터가 아무 많다면 단 몇 %만 떼어 놓아도 전체 데이터를 대표하는데 문제가 없다.

### 교차 검증
: 교차 검증은 검증 세트를 떼어 내어 평가하는 과정을 여러 번 반복하는 것    
<img width="80%" src="https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/73346564/e272c58f-0bd2-40c7-8dbb-5c04875ecceb"/>

> ### 3-폴드 교차 검증
> 훈련 세트를 세 부분으로 나눠서 교차 검증을 수행하는 것.
> 통칭 k-폴드 교차 검증(k-fold cross validation)이라고 하며, 훈련 세트를 몇 부분으로 나누냐에 따라 다르게 부른다
> k-겹 교차 검증이라고도 부른다.    
> 보통 5-폴드 교차 검증이나 10-폴드 교차 검증을 많이 사용한다
> 이렇게 하면 데이터의 80~90% 까지 훈련에 사용할 수 있으며, 검증 세트가 줄어들지만 각 폴드에서 계산한 검증 점수를 평균하기 때문에 안정된 점수로 생각한다.

- cross_calidate() 교차 검증 함수 in 사이킷런
  : 평가할 모델 객체를 첫 번째 매개변수로 전달. 그다음 앞에서처럼 직접 검증 세트를 떼어 내지 않고 훈련 세트 전체를 cross_validate() 함수에 전달한다.

> ### 교차 검증의 최종 점수는 test_score 키에 담긴 5개의 점수를 평균하여 얻을 수 있다
> ### 이름은 test_score지만 검증 폴드의 점수이다!!

### 하이퍼파라미터 튜닝
> 사람의 개입 없이 하이퍼파라미터 튜닝을 자동으로 수행하는 기술을 'AutoML' 이라고 부른다.

1. 탐색할 매개변수를 지정
2. 훈련 세트에서 그리드 서치를 수행하여 최상의 평균 검증 점수가 나오는 매개변수 조합을 찾기. 이 조합은 그리드 서치 객체에 저장된다.
3. 그리드 서치는 최상의 매개변수에서 (교차 검증에 사용한 훈련 세트가 아니라) 전체 훈련 세트를 사용해 최종 모델을 훈련한다.
4. 위의 모델도 그리드 서치 객체에 저장된다.

### 랜덤 서치
: 매개변수 값의 목록을 전달하는 것이 아니라 매개변수를 샘플링할 수 있는 확률 분포 객체를 전달한다.

# 5-3. 트리의 앙상블

### 앙상블 학습
: 정형 데이터를 다루는 데 가장 뛰어난 성과를 내는 알고리즘이며, 여러 개의 분류기를 생성하고 각 예측들을 결합함으로써 보다 정확한 예측을 도출하는 기법      
이 알고리즘은 대부분 결정 트리를 기반으로 만들어져 있다. 


### 랜덤 포레스트
: 데이터를 만드는 방법(부트스트랩 샘플)    
랜덤 포레스트의 표본 추출방식은 부트스트랩 샘플 이라고 부른다.  

#### 부트스트랩 샘플
: 입력한 훈련 데이터에서 랜덤하게 샘플을 추출하여 훈련 데이터 만들기. 이때 한 샘플이 중복되어 추출될 수 있다    
기본적으로 부트스트랩 샘플은 훈련 세트의 크기와 같게 만든다.   
ex. 1000개의 가방에서 100개씩 샘플을 뽑는다면, 먼저 1개를 뽑고, 뽑았던 1개를 다시 가방에 넣는다.

<img width="50%" src="https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/73346564/228b17e2-485e-459f-88ef-ab2ebb3ef130"/>

> ### 부트스트랩이란?
> 데이터 세트에서 중복을 허용하여 데이터를 샘플링하는 방식을 의미

### RandomForestClassifier
: 전체 특성 개수의 제곱근만큼이 특성을 선택한다   
즉, 4개의 특성이 있다면 노드마다 2개를 랜덤하게 선택하여 사용한다. 다만 회귀모델인 RandomForestRegressor는 전체 특성을 사용한다. 

<img width="50%" src="https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/73346564/f48e6c3b-9d16-456f-9eca-8461d7675742"/>

회귀일 때는 단순히 각 트리의 예측을 평균한다.

- 랜덤 포레스트는 랜덤하게 선택한 샘플과 특성을 사용하기 때문에 훈련 세트에 과대적합되는 것을 막아주고 검증 세트와 테스트 세트에서 안정적인 성능을 얻을 수 있다.
- 랜덤 포레스트가 특성의 일부를 랜덤하게 선택하여 결정 트리를 훈련하기 때문에, 각 특성마다 중요도가 변한다.

> ### RandomForestClassifier에서의 자체적으로 모델을 평가하는 점수
> 랜덤 포레스트는 훈련 세트에서 중복을 허용하여 부트스트랩 샘플을 만들어 결정 트리를 훈련하는데, 이때 부트스트랩 샘플에 포함되지 않고 남는 샘플이 있다
> 이런 샘플을 OOB(out of bag)샘플 이라고 하며 이 남는 샘플을 사용하여 부트스트랩 샘플로 훈련한 결정 트리를 평가할 수 있다(마치 검증 세트의 역할)

### 엑스트라 트리
: 랜덤 포레스트와 엑스트라 트리의 차이점은 부트스트랩 샘플을 사용하지 않는다는 것이다.  
즉, 각 결정 트리를 만들때 전체 훈련 세트를 사용한다. 대신 노드를 분할할 때 가장 좋은 분할을 찾는것이 아니라 무작위로 분할한다.   
-> 최대한 귤일하게 데이터 세트가 구성되도록 분할하는 것이 중요하다. but, 엑스트라 트리에서는 무작위 분할중 가장 좋은 것을 분할 규칙으로 선택한다.

- 장.단점 : 엑스트라 트리가 무작위성이 좀 더 크기 때문에 랜덤 포레스트보다 더 많은 결정 트리 훈련. 하지만, 노드를 분할하기 때문에 빠른 계산 속도가 장점이다.

> 결정 트리는 최적의 분할을 찾는데 시간을 많이 소모한다. 특히 고려해야 할 특성의 개수가 많을 때 더 그렇다. 만약 무작위로 나눈다면 훨씬 빨리 트리를 구성할 수 있다.

### 그레이디언트 부스팅
: 깊이가 얕은 결정 트리를 사용하여 이전 트리의 오차를 보완하는 방식으로 앙상블 하는 방법이다.   
깊이가 얕은 결정 트리를 사용하기 때문에 과대적합에 강하고 일반적으로 높은 일반화 성능을 기대할 수 있다.   
(분류 - 로지스틱 손실 함수 , 회귀 - 평균 제곱 오차함수 사용)   

- 가장 낮은 곳을 찾아 내려오는 방법은 모델의 가중치와 절편을 조금씩 바꾸는 것
- 그래디언트 부스팅은 결정 트리를 계속 추가하면서 가장 낮은 곳을 찾아 이동
> 낮은 곳으로 천천히 조금씩 이동하기 때문에 결정트리의 개수를 늘려도 과대적합에 강하다

### 히스토그램 기반 그레이디언트 부스팅
: 정형 데이터를 다루는 머신러닝 알고리즘 중에서 가장 인기가 높은 알고리즘 이다.  
입력 특성을 256개의 구간으로 나눈다. 256개의 구간 중에서 하나를 떼어 놓고 누락된 값을 위해서 사용한다.    
즉, 입력에 누락된 특성이 있더라도 이를 따로 전처리 할 필요가 없다.

#### permutation_importance() 함수
: 이 함수는 특성을 하나씩 랜덤하게 섞어서 모델의 성능이 변화하는지를 관찰하여 어떤 특성이 중요한지를 계산

## 최종 정리
: 결정 트리 기반의 앙상블 학습은 강력하고 뛰어난 성능을 제공하기에 인기가 높다  
랜덤 포레스트는 대표적인 앙상블 학습 중 하나이다. 성능이 좋고 안정적이기에 첫 번째로 시도해 볼 수 있는 앙상블 학습 중 하나이다.   
랜덤 포레스트는 결정 트리를 훈련하기 위해 부트스트랩 샘플을 만들고 전체 특성 중 일부를 랜덤하게 선택하여 결정 트리를 만든다  

엑스트라 트리는 랜덤 포레스트와 비슷하지만, 부트스트랩 샘플을 사용하지 않고 노드를 분할할 때 최선이 아니라 랜덤하게 분할한다.    
이러한 특성 때문에 랜덤 포레스트보다 훈련 속도가 빠르지만 더 많은 트리가 필요하다   

그레이디언트 부스팅은 깊이가 얕은 트리를 연속적으로 추가하여 손실 함수를 최소화 하는 앙상블 방법이다.    
성능이 뛰어나지만 병렬로 훈련할 수 없기에 랜덤 포레스트나 엑스트라 트리보다 훈련 속도가 조금 느리다.   
학습률 매개변수를 조정하여 모델의 복잡도를 제어할 수 있으며, 학습률 매개변수가 크면 복잡하고 훈련 세트에 과대적합된 모델을 얻을 수 있다.   

히스토그램 기반 그레이디언트 부스팅은 훈련 데이터를 256개의 구간으로 변환하여 사용하기 때문에 노드 분할 속도가 매우 빠르다  
