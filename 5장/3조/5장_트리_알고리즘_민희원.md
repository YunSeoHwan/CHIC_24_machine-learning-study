# 5장 트리 알고리즘
### 누락된 값이 있을 경우
- 데이터를 버리거나 평균값으로 채운 후 사용할 수 있음.
- 어떤 방식이 최선인지는 미리 알기 어려우므로 모두 시도해 본 후 찾기

## 결정트리 (Decision Tree)
데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리 기반의 분류 규칙을 만드는 것.
- 어떤 기준으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 크게 좌우함.
- 하지만 규칙이 많다는 것은 좀 더 예측을 위한 학습이 잘 된다고 말할 수 있음과 동시에 복잡하다는 의미이며, **과적합**으로 이어질 수 있음.
- 즉, 트리의 깊이(Depth)가 깊어질수록 예측 성능이 저하될 가능성이 높기 때문에 적절한 값을 찾아야 함.
- 정보의 균일도를 측정하는 방법으로는 엔트로피를 이용한 **정보이득**(Information Gain) 지수와 **지니계수**가 있음.

### 정보이득과 지니계수
- 정보이득은 엔트로피라는 개념을 기반으로 한다. **엔트로피**란 <u>주어진 데이터의 집합의 혼잡도</u>를 의미하는데, 서로 다른 값들이 섞여있으면 엔트로피가 높고 같은 값으로 섞여있으면 엔트로피가 낮음. 여기서 정보이득지수는 **1 - (엔트로피 지수)** 를 의미.
- 지니계수는 낮을수록 데이터의 균일도가 높고, 지니계수가 높으면 균일도가 낮음을 의미.
- 불순도가 낮다 = 지니계수가 낮다 = 엔트로피가 낮다 = 균일도가 높다

   주의할 점: 클래스가 섞여있다고 나쁜 것은 아님. (불순도가 0이 아니라고 해서 나쁜 것은 아님)
- 결정트리를 평가하는 지표는 정확도, 정밀도, 재현율, f1 score 등을 통해 이루어짐.

   f1 score : 정밀도(Precision)과 재현율(Recall)의 기하평균(Feometric Mean) 값으로, 모델의 정밀도와 재현율을 모두 고려한 평가 지표
- **특성 중요도**란 결정 트리에 사용된 특성이 불순도를 감소하는데 기여한 정도를 나타내는 값. 특성 중요도를 계산할 수 있는 것이 결정 트리의 또다른 큰 장점.

## 교차 검증과 그리드 서치
- **교차 검증(Cross-validation)** 이란 데이터를 여러 번 반복해서 나누고 여러 모델을 학습하여 성능을 평가하는 방법임.
- 이렇게 하는 이유는 데이터를 학습용/평가용 데이터 세트로 여러 번 나눈 것의 평균적인 성능을 계산하면, 한 번 나누어서 학습하는 것에 비해 **일반화된 성능**을 얻을 수 있기 때문임. 이 때문에 교차 검증은 기존 대비 안정적이고 뛰어난 통계적 평가방법으로 평가 받음.
- 3-폴드 교차 검증은 훈련 세트를 세 부부능로 나눠서 교차검증을 수행하는 방식임. 통칭 K-fold cross validation이라고 함.

[+ 교차 검증](https://velog.io/@kkamz/DS%EB%A9%B4%EC%A0%91-%EB%8C%80%EB%B9%84-Cross-Validation%EC%9D%80-%EB%AC%B4%EC%97%87%EC%9D%B4%EA%B3%A0-%EC%96%B4%EB%96%BB%EA%B2%8C-%ED%95%B4%EC%95%BC%ED%95%98%EB%82%98%EC%9A%94-xl98te3r)

- **그리드 서치(Grid Search, 격차 탐색)** 은 모델 하이퍼 파라미터에 넣을 수 있는 값들을 순차적으로 입력한 뒤에 가장 높은 성능을 보이는 하이퍼 파라미터 값을 찾는 탐색 방법임.
- 하이퍼 파라미터는 **정해진 최적의 값이 없음.** 휴리스틱한 방법이나 경험 법칙(Rules of thumb)에 의해 결정하는 경우가 많음. 베이지안 옵티미제이션과 같이 자동으로 하이퍼 파라미터를 선택해주는 라이브러리도 있음.
- Clustering에서 최적의 군집수를 정하는 방식 : 
[Elbow Method, silhouette](https://steadiness-193.tistory.com/285)

## 트리의 앙상블
앙상블은 프랑스어로 조화, 통일을 뜻하는 프랑스어임.

기계학습에서의 앙상블도 이와 비슷하며, 여러 개의 Weak learner들이 모여 투표(Voting)를 통해 더욱 더 강력한 strong learner를 구성함.

많은 모델이 있기에, 한 모델에서 예측을 엇나가게 하더라도 어느정도 보정이 됨. 즉, 보다 일반화된 모델이 완성되는 것임.

#### voting의 종류
- **하드보팅**(Hard voting) : 각 weak learner들의 예측 결과값을 바탕으로 다수결 투표하는 방식.
- **소프트보팅**(Soft voting) : weak learner들의 예측 확률값의 평균 또는 가중치 합을 사용하는 방식.

앙상블은 학습 방식에 따라 크게 **배깅(Bagging), 부스팅(Boosting), 스태킹(Stacking)** 으로 나눌 수 있음.

### 배깅(Bagging)
- 배깅은 **B**ootstrap **Agg**regat**ing**의 약자이며, 부트스트랩(Bootsrap)을 이용함.
- 부트스트랩이란 주어진 데이터셋에서 random sampling하여 새로운 데이터셋을 만들어내는 것을 의미.
- 부트스트랩을 통해 만들어진 여러 데이터셋을 바탕으로 weak learner를 훈련시킨 뒤, 결과를 voting함.
- 대표적인 예시로 **Random Forest**가 있음.

### 부스팅(Boosting)
- 부스팅은 반복적으로 모델을 업데이트 함.
- 이 때 이전 iteration의 결과에 따라 데이터셋 샘플에 대한 가중치를 부여함.
- 결과적으로, 반복할 때마다 각 샘플의 중요도에 따라 다른 분류기가 만들어지게 됨.
- 최종적으로는 모들 iteration에서 생성된 모델의 결과를 voting함.
- Boosting은 다시 **Adaptive Boosting(AdaBoost)와 Gradient Boosting Model(GBM)** 계열로 나눌 수 있음.

### 스태킹(Stacking)
- weak learner들의 예측결과를 바탕으로 meta learner로 학습시켜 최종 예측값을 경정하는 것을 말함.
- meta learner 또한 학습이 필요하며, 이 때 사용되는 데이터는 training data에 대한 각 weak learner들의 예측 확률값의 모음임.
- 과적합 방지를 위해 주로 k-fold cross validation을 이용함.

[+ 참고한 사이트](https://tyami.github.io/machine%20learning/ensemble-1-basics/)