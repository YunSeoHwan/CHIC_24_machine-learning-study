### <네 번째 스터디 – 2024.02.01> 
### 범위: p.176~p.241

##### decision tree에서 feature를 설정하는 기준?
decision tree의 spliter 속성에서 best : 불순도를 가장 줄일 수 있는 decision tree를 찾는 것.
spliter에 내장된 알고리즘: CART 알고리즘: greedy nethod의 일종(현재 상황에서 최선을 찾는 것)
##### decision tree의 장점/주의점
- decision tree의 depth가 무한대이면 무조건 성능이 좋을 것 (과대적합) - 주의해야 함.
- explainable ai : 점수가 왜 잘 나왔는지를 알 수 있음.
- 딥러닝과의 비교: 딥러닝에서의 말 사진 분류 사례: 문제가 될 수 있다 decision tree에서는 분류의 기준이 모두 적혀있다.
##### support vector machine
- kenel method를 통해 데이터와 boundary를 왜곡시켜서 일반화 성능을 강화할 수 있음. : 나누기에 좋게 만듦.
### <다섯 번째 스터디 - >
### 범위: p.242~337

##### 하이퍼파라미터 튜닝에서의 최적화
- 최적의 매개변수 조합을 찾는다는 것의 의미
-> 데이터의 특성을 잘 보존하면서. 최적은 "일반화" 성능을 높이는 것. unseened data에 대해서도 적용이 되는가. 따라서, 성능을 무조건 높이는 것이 아니라, 훈련세트의 점수와 테스트세트의 점수를 모두 고려하여 일반화 성능을 보존하며 점수를 높이는 것이므로 최적의 매개변수 조합을 찾아나가야 함( 예를 들어, depth를 최대한으로 하는 것이 무조건적으로 좋은 것이 아님)
##### 앙상블을 사용하는 이유와 앙상블의 학습방식 bagging, boosting
- - 앙상블을 사용하는 이유(목적): bias와 variance의 trade-off한 특성이 있는데, optiaml한 point를 찾는 것이 목적임. 이때, 앙상블은 다양한 개별 모델들을 고려하는 것이며, 그 모델들을 어떻게 결합할지가 중요함. (프랑스어 Ensemble: 전체적인 어울림)
- bagging: 복원추출을 진행해서 데이터 다양성을 확보하는 것 
- 대표적인 예시 : random forest 기법, voting
※ voting 
- hard voting : weak learner들을 다수결 voting
- soft voting : weak learner들을 평균/가중치 합을 사용
- bagging은 일반화 성능이 올라가고 variance를 줄일 수 있음 -> 다양성 확보 (랜덤포레스트도 마찬가지)
- boosting : 순차적 학습을 통해 다음 학습 결과의 성능을 높임 / 랜덤 샘플링(중복 가능, 복원 추출) / OOB / 일반화 성능의 보장 /  
- boosting은 bias 높이기 위함. 데이터가 나오는데 10번 데이터에서 틀리면 다음 학습때에는 가중치를 줌. -> 부스팅
- boosting에서는 데이터의 다양성보다는 성능을 우선시  
##### 비지도학습에서의 과대적합
- 비지도학습에도 과대적합이 존재함. 책에서는 cluster를 기준으로 다뤘는데, clustering은 2차원에서 가능(시각화를 위해서). k-means는 density한 군집을 잡지 못함 (예를들어, 도넛모양,태극모양) 이유: Euclidian distance를 사용하기 때문에.
- 어떠한 데이터에 대해 판단해야 하는데, 실제로는 아니다. 인위로 label을 붙이는 것. 사후검증이 되면 성능을 판단할 수 있음. "dbscan" -> 기계학습/빅처응 확인해보기.
※ 준지도학습은 절반으로 학습하고 나머지는 검증.
##### RandomForestClassifier의 작동원리
- 특성이 16개이면 4개의 특성을 이용해서 4개의 트리를 만드는 것 16C4개. 기본적으로 이렇게 작동함.
##### greedy search
- 지정한 범위 내에서의 최고점을 찾을 수 있지만 local minimum일 수 있다. 그러나 설정한 범위가 크면 시간적으로도 오래 걸릴 수 있음
bruth force와 비슷해질 수 있다. 많은 조합을 테스트하기에는 어려움이 있음.
※ 최근에는 Auto ML 기반의 최적화를 많이 사용함. optuna, pycarrot
##### PCA에서 차원축소를 진행하면 이미지가 깨지는 것 같음
- PCA자체의 주요목적: 분산을 최대한 보존하면서 차원을 줄이는 HYPER PLANE을 찾는 것. 차원을 줄이게 되면 당연히 고유한 특성을 어느정도는 손실이 있을 것. 그러나 최소화 되어도 득보다 실이 많으면 실행하는 것.
원상복구가 되지는 않음.
- 예를들어, 10차원 -> 2차원 -> 10차원 에서 원상복구가 되지 않는 경우도 있음. "reconstruction error"
##### 차원축소의 필요여부에 대한 판단기준
- 알 수 없음. 우선, DATA ROW(data양)가 많아서 시간이 많이 걸리는지, feature 수가 많아서 시간이 많아서 걸리는지 확인해야 함. feature의 갯수가 지나치게 많다고 생각되면 PCA
- feature 10개 성능 0.7 feature 5개 성능 0.69이면 줄이는게 아마 좋을 것. "사후적인 분석". 물론 feature수가 너무 많으면 하는 것이 좋겠지만.. 
- reconstruction error를 확인하면 얼마의 중요도로 pca가 되었는지 확인 할 수 있음
 