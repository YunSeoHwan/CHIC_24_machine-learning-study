# 트리 알고리즘(5-2, 5-3) - 5주차
**5-3**까지 내용을 정리함을 서두에 밝힙니다.<br>
책 내용에서 보다 깊이 있게 다루면 좋을 요소 위주의 정리를 진행했습니다. 따라서 책 전체 내용의 기술되어 있지 않을 수 있습니다. 궁금한 사항이나, 잘못된 부분이 있다면, 언제든 issue에 의견을 남겨주시면 감사하겠습니다. 해당 내용은 **기계학습  - 이상민 교수님**자료와 **핸즈온 머신러닝 3판**을 참고하여 만들었습니다.

## Ensemble(앙상블)

### Background
Ensemble이 무엇인지 설명하기에 앞서 간략한 background를 짚고 넘어가겠습니다. 기본적으로 **No Free Lunch theorem**에 따르면 특정한 문제에 최적화된 알고리즘은 다른 문제에서는 그렇지 않다는 것을 수학적으로 증명하고 있습니다. 즉, 어떤 알고리즘도 모든 상황에서 모든 데이터에 대해 다른 알고리즘보다 우월하다는 결론을 내릴 수 없습니다. 즉, 각각의 개별 model을 조합하여 대부분의 문제에서 좋은 성능을 달성해 보는 것은 어떨까? 라는 것이 ensemble의 배경이라고 볼 수 있습니다.
![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/48356954/d909ede1-9abd-4f94-b010-6d74559e66c7)
![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/48356954/ae50afd4-68b8-4d77-a66b-ebc59d3828aa)
<br><br>

### Ensemble의 목적
Ensemble은 앞서 언급했듯이, 개별 모델을 조합하여 하나의 모델을 생성합니다. 그렇기 때문에, 각각의 모델이 가지는 장점들을 적절하게 포함할 수 있습니다. 즉, 모델의 **bias**, **variance**를 이론적으로 모두 낮출 수 있습니다. 이 둘은 **trade-off**관계를 가지기 때문에 적절한 optimal을 찾는 것이 중요합니다. 즉, Ensemble은 어떻게 다양성을 확보함과 동시에 최종적인 개별 모델을 어떻게 결합할지가 중요한 아이디어라고 볼 수 있습니다.
![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/48356954/2da0f438-8b47-4dd3-806a-9ad05adba7b3)

<br><br>
### Bagging
앞서 언급한 모델의 다양성을 확보하는 방법입니다. **Bagging**은 Bootstrapp Aggregating의 약자로 resampling을 통한 다양한 dataset을 확보하여 모델의 다양성을 확보하는 방식입니다. 여기서 resampling은 random하게 복원추출로 이루어지는데, 이렇게 생성된 각각의 dataset을 bootstrapp이라고 부릅니다. 즉, 이러한 bootstrap을 이용하여 모델의 다양성을 확보합니다.
![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/48356954/725ced27-f0ef-4136-b1a1-99535861d80c)
<br><br>
이렇게 확보한 다양한 모델을 어떻게 Aggregating할까요? 여러 방법이 존재하지만, 자주 사용되는voting에 대해 소개하겠습니다. voting에도 크게 soft voting, weight voting이 존재합니다. <br><br>
먼저 **soft voting**은 **majority voting**으로도 불리며, 각각의 개별 모델이 가장 많이 예측한 값을 최종 모델의 결과로 가져가는 방식입니다. 아래의 그림을 보면 이해하기 쉽습니다.
![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/48356954/21d62279-295b-47c7-a841-8653eb4b9fc7)
<br><br>
**weight voting**의 경우, 각각의 개별 모델이 어느정도의 확률로 예측을 했는지까지를 고려합니다. 가령, 대부분의 모델이 1로 예측을 했음에도 그 확률이 0.5와 같은 임계치에 가깝다면 그 값을 신뢰하기란 어렵습니다. 이러한 확률을 가중치로 반영한 방식이 바로 weight voting입니다. 
![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/48356954/c7ebd57c-a67d-4a23-b4ed-3761362bc19b)
