## 군집 알고리즘

지도학습의 목적
- Find a function f that explains the relationship between the input X and the output Y
비지도학습의 목적
- Explore the features of the input X

Core of Clustering Algorithm
- minimize the intra-cluster variance
- maximize the inter-cluster variance

Classification vs Clustering
- 분류는 사전 정의된 범주가 있는 label로부터 예측 모델을 학습함.
- 군집화는 unlabeled data로부터 최적의 그룹을 탐색하며, 예측이 목적이 아님.

군집화 시 주의사항
- 어떤 거리 척도로 유사도를 측정하는가? (유클리드, 맨허튼, 마할라노비스,...)
- 어떤 군집화 알고리즘을 사용할 것인가? (계층, 분리, 자기조직, 분포 기반, ...)
- 어떻게 최적의 군집 수를 결정하는가?
- 어떻게 군집화 결과를 측정/평가하는가? (SSE, silhouette, ...)
- 
## k-mean 알고리즘

절차
Select K points as the initial centroids # K는 하이퍼파라미터
Repeat
  Form K clusters by assigning all points to the closest centroid.
until The centroids don't change.

(군집 파트는 이론적인 내용이 별로 없어서 후에 코드를 정리하여 올리겠다.)


## 주성분 분석

차원의 저주를 먼저 생각해보자.
데이터는 여러 정보을 통해 표현할 수 있고, 정보가 많을수록 더욱 정밀히 표현 가능하다.
예를 들어, 한 반의 친구들을 얼굴에 대한 설명으로 구분하는 경우,
'눈 크기', '눈 너비', '눈썹 각도', '콧볼 너비', '피부색', '얼굴형', '점 개수'로
무려 7가지 특성을 이용하면 각각의 개인에 대해 정확한 구별이 가능해질 것이며, 
이는 `7차원 공간 상의 한 점으로 데이터를 표현할 수 있다`로 해석할 수 있다.

물론 정보가 많을 수록 정확한 구별은 되겠지만, 달리 말하면 
`한 데이터를 표현하기 위해 수많은 차원이 요구됨`을 의미하다.
고차원 공간을 표현하기 위해 필요한 데이터는 기하급수적으로 증가하며, 데이터 간 거리 역시 기하급수적으로 증가한다. (차원의 저주로 유클리드 거리는 '직선 거리'로서의 본연의 의미를 잃는다.)
결국 Overfitting 문제까지 이어진다.
그렇다면 해결책으로 두 가지 정도를 떠올릴 수 있는데, 첫 번째는 데이터 수를 늘리는 것이다.\
고차원 공간을 표현할 수 있도록 데이터 수를 충분히 늘리면 되지 않을까? 이론상 균일한 분포를 이루는 데이터셋이 있다고 생각해보자.
길이가 1인 각 축에 0.1 간격으로 각 데이터를 배치시키면 1차원에 10개, 2차원에 100개이니,
위의 예시처럼 7개 특성만 가져도 10,000,000개의 데이터가 필요하다. 매우 어려울 것이다.\
 두 번째는 차원을 축소하는 것이다. 
 위에서 '눈 크기'와 '눈 너비' 같은 경우만 보더라도 어느 정도 상관관계가 있을 것임을 알 수 있다.(다중 공선성 문제)
 feature selection과 feature extraction(PCA가 대표적)을 통해 정리하면 학습 시간 감소, 모델 성능 향상 외에도 데이터를
직관적, 시각적으로 이해하는 데 도움이 될 것이다. 

##### <i>차원 축소 중에서 대표적인 수단이 바로 PCA인데, 하단에 수식을 조금 많이 적었다. 교재나 블로그에 보면, '분산이 큰 방향이 분포를 잘 설명할 것이다' 등을 의미적으로 설명했고, 피상적으로 이해하는 데는 도움이 됐지만, 조금 깊이있게 들어가면 왜 공분산과, 고유값 분해, 차원 축소가 왜 연결되어 있는지를 알기 어려웠고, 지금도 증명 과정을 다 이해하지는 않았지만(라그랑주 승수법 등), 사진으로 첨부했다.</i>
![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/129747097/00301ba8-3d09-4c0b-a57f-3104b5ca675e)
![image](https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/129747097/88bc9ac6-4d51-4374-a2b1-f8c4579cf11d)

'길이가 가장 긴 화살표를 선택해야 점들이 최대한 안 겹칠 것이다. 분산이 크다는 건 밀집되어 있지 않고 퍼져있으니 설명하기 좋은 축을 고르려면
분산이 가장 큰 방향을 가리켜야 한다'라고 친절하게는 나와있지만 무언가 갈증이 해소되지 않아 찾다보니 선형대수학에서 해답을 얻었다.(대충 게비스콘 짤)\

데이터로부터의 COST를 최소화하는 과정에서 유도된 공분산 행렬은 대칭행렬이고,
이는 고유벡터의 직교행렬과 고유값의 정방행렬로 대각화가 가능하다.(공분산 행렬의 고유값 분해)
이때 고유값은 내림차순으로 정리될 테니, 큰 값을 갖는 고유값들에 해당하는 고유벡터들이 바로 분산이 큰 방향들인 것이다.(PCA_1, PCA_2, ...)


