# 용어
- 군집 : 비슷한 샘플 끼리 그룹을 모으는 작업
- 클러스터 : 군집 알고리즘에서 만든 그룹
- k-평군 알고리즘 : 처음에 랜덤하게 클러스터 중심을 정하고 클러스터 만듦. 그 다음 클러스터의 중심을 이동하고 다시 클러스터를  
      만드는 식으로 반복해서 최적의 클러스터를 구성한다.
- 클러스터 중심 : k-평균 알고리즘이 만든 클러스터에 속한 샘플의 특성 평균값이다. 센트로이드(centroid)라곧 불린다.
- 엘보우 방법 : 최적의 클러스터 개수를 정하는 방법 중 하나이며, 이너셔는 클러스터 중심과 샘플 사이 거리의 제곱 합이다.
    클러스터 개수에 따라 이너셔 감소가 꺾이는 지점이 적절한 클러스터 개수 k가 될 수 있다. 그래프 모양을 따서 엘보우 방법이라고 한다

# 6-1. 군집 알고리즘
- 비지도 학습
  : 타깃이 없을 때 사용하는 머신러닝 알고리즘

> #### 이미지 색상
> 0 ~ 255 숫자중, 0 에 가까울수록 검은색 255에 가까울수록 밝은색을 나타낸다

> #### 컴퓨터는 왜 255에 가까운 바탕에 집중하는지?
> 알고리즘이 어떤 출력을 만들기 위해 곱셈, 덧셈을 한다. 픽셀값이 0이면 출력도 0이 되어 의미가 없다. 픽셀값이 높으면 출력값도 커지기 때문에 의미를 부여하기 좋다.

- axis = 0 : 행 방향으로 계산 / axis = 1 : 열 방향으로 계산

# 6-2. k-평균
- k-평균 군집 알고리즘 : 평균값을 자동으로 찾아준다
- 평균값이 클러스터의 중심에 위치하기 떄문에 클러스터 중심 또는 센트로이드 라고 부른다.

#### k-평균 알고리즘 소개
1. 무작위로 k개의 클러스터 중심을 정한다
2. 각 샘플에서 가장 가까운 클러스터 중심을 찾아 해당 클러스터의 샘플로 지정한다
3. 클러스터에 속한 샘플의 평균값으로 클러스터 중심을 변경한다
4. 클러스터 중심에 변화가 없을 때까지 2번으로 돌아가 반복한다

<img width="80%" src="https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/73346564/305ae267-fcd8-41f0-a1ea-b406d38aa223"/>

위의 그림을 예로 들어보자.
(1) 먼저 3개의 클러스터 중심(빨간 점)을 랜덤하게 지정한다.
(2) 클러스터 중심에서 가장 가까운 샘플을 하나의 클러스터로 묶는다
(3) 클러스터의 중심을 다시 계산하여 이동시킨다.
(4) 계산 이후 가장 가까운 샘플을 다시 클러스터로 묶는다
(5) 다시 한번 클러스터 중심을 계산한다
(6) 그 다음 빨간점을 클러스터의 가운데 부분으로 이동시킨다
(7) 이동된 클러스터 중심에서 다시 한번 가까운 샘플을 클러스터로 묶는다
(8) 중심에서 가장 가까운 샘플이 이전 클러스터와 동일시 알고리즘 종료

### KMeans 클래스
: K는 묶을 군집(클러스터)의 개수를 의미하고 means는 평균을 의미한다.
즉, K개의 군집으로 묶는다는 의미이다.

### 최적의 K 찾기
: k-평균 알고리즘의 단점 중 하나는 클러스터 개수를 사전에 지정해야 한다

#### 엘보우(elbow)
: 적절한 클러슽 개수를 찾기 위한 대표적인 방법    
엘보우 방법은 클러스터 개수를 늘려가면서 이너셔의 변화를 관찰하여 최적의 클러스터 개수를 찾는 방법이다.   

#### 이너셔(inertia)
: 클러스터 중심과 클러스터에 속한 샘플 사이의 거리를 재며, 이 거리의 제곱 합을 이너셔 라고 부른다.   
이너셔는 클러스터에 속한 샘플이 얼마나 가깝게 모여 있는지를 나타내는 값으로 생각할 수 있다.   
알반적으로 클러스터 개수가 늘어나면 클러스터 개개의 크기는 줄어들기 때문에 이너셔도 줄어든다    
     
클러스터 개수를 증가시키면서 이너셔를 그래프로 그리면 감소하는 속도가 꺾이는 지점이 있다  
이 지점부터는 클러스터 개수를 늘려도 클러스터에 잘 밀집된 정도가 크게 개선되지 않는다  
즉, 이너셔가 크게 줄어들지 않는다. 이 지점이 마치 팔꿈치 모양이어서 엘보우 방법이라 부른다 

<img width="80%" src="https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/73346564/06d76f20-1014-48d3-adee-a616e7c9c057
"/>

# 6-3. 주성분 분석(PCA)
- 차원과 차원 축소
### 주성분 분석 소개
: 주성분 분석(PCA)은 데이터에 있는 분산이 큰 방향을 찾는 것으로 이해 할 수 있다.
분산은 데이터가 널리 퍼져있는 정도를 말한다. 분산이 큰 방향이란, 데이터를 잘 표현하는 어떤 벡터라고 생각 할 수 있다.   
   
#### 이때 중요한 것은, 분산이 큰 방향을 찾는 것이 중요하다

> 실제로 사이킷런의 PCA 모델을 훈련하면 자동으로 특성마다 평균값을 빼서 원점을 맞춰준다.

- 주성분 벡터 : 우너본 데이터에 있는 어떤 방향
- 주성분 벡터의 원소 개수는 원본 데이터셋에 있는 특성 개수와 같다
- 하지만, 원본 데이터는 주성분을 사용해서 차원을 줄일 수 있다
- 주성분은 원본 차원과 같고 주성분으로 바꾼 데이터는 차원이 줄어든다는 것!!
#### 주성분이 가장 분산이 큰 방향이기 때문에 주성분에 투영하여 바꾼 데이터는 원본이 가지고 있는 특성을 가장 잘 나타내고 있다    
   
- 첫 번째 주성분을 찾은 다음 이 벡터에 수직이고 분산이 가장 큰 다음 방향을 찾는다.
- 이 벡터가 두 번째 주성분이다

> 기술적인 이유로 주성분은 원본 특성의 개수와 샘플 개수중 작은 값만큼 찾을 수 있다. 일반적으로 비지도 학습은   
> 대량의 데이터에서 수행하기 때문에 원본 특성의 개수만큼 찾을 수 있다고 한다

### 원본 데이터 재구성
: 특성을 줄이게 되면 어느정도 손실이 발생할 수 밖에 없다  
하지만, 최대한 분산이 큰 방향으로 데이터를 투영했기 때문에 원본 데이터를 상당 부분 재구성할 수 있다

### 설명된 분산
: 주성분이 원본 데이터의 분산을 얼마나 잘 나타내는지 기록한 값을 설명된 분산이라고 한다.

<img width="80%" src="https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/73346564/af42ade9-c06c-4bc3-ad90-f02403c896ac"/>


p.326
