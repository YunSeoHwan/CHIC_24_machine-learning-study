09-1 순차 데이터와 순환 신경망  

순차 데이터 / 텍스트나 시계열 데이터와 같이 순서에 의미가 있는 데이터
피드포워드 신경망 / 입력 데이터의 흐름이 앞으로만 전달되는 신경망

순환 신경망 / 일반적인 완전 연결 신경망과 거의 비슷 
타임스텝 / 샘플을 처리하는 한 단계
셀 / 순환 신경망에서 층을 부르는 말 
-> 셀의 출력을 은닉 상태라 부름 

09-2 순환 신경망으로 IMDB 리뷰 분류하기

말뭉치 / 자연어 처리에서 사용하는 텍스트 데이터의 모음 -> 훈련 데이터셋
토큰 / 텍스트에서 공백으로 구분되는 문자열 
단어 임베딩 / 정수로 변환된 토큰을 비교적 작은 크기의 실수 밀집 벡터로 변환 
-> 밀집 벡터는 단어 사이의 관계를 표현할 수 있기 때문에 자연어 처리에서 좋은 성능을 보여줌 

pad_sequences() / 시퀀스 길이를 맞추기 위해 패딩 추가 -) (샘플 개수, 타임스텝 개수) 크기의 2차원 배열을 기대
-> maxlen 매개변수로 원하는 시퀀스 길이 지정 가능 / 이 값보다 크면 잘리고 짧은 시퀀스는 패딩 
-> padding 매개변수는 패딩을 추가할 위치 지정 / 기본값인 pre는 시퀀스 앞에 패딩 추가 post는 뒤에 패딩 추가
-> tyrncating 매개변수는 긴 시퀀에서 잘라버릴 위치 지정 / pre 앞 post 뒤 시퀀스 부분을 자름 

to_categorical() / 정수 시퀀스를 원-핫 인코딩으로 변환 
-> 토큰을 원-핫 인코딩하거나 타깃값을 원-핫 인코딩할 때 사용 
-> num_classes 매개변수에서 클래스 개수를 지정할 수 있음 / 지정x -> 데이터에서 자동으로 찾음 

SimpleRNN / 케라스의 기본 순환층 클래스 
-> 첫 번째 매개변수에 뉴런 개수 지정 
-> activation 매개변수에서 활성화 함수 지정 (기본값 tanh)
-> dropout 매개변수에서 입력에 대한 드롭아웃 비율을 지정
-> return_sequences 매개변수에서 모든 타임스텝의 은닉 상태를 출력할 지 결정 (기본값 false)

Embedding / 단어 임베딩을 위한 클래스 
-> 첫 번째 매개변수에서 어휘 사전의 크기 지정 
-> 두 번째 매개변수에서 Embedding 층이 출력할 밀집 벡터의 크기 지정 
-> input_length 매개변수에서 입력 시퀀스의 길이 지정 이 매개변수는 Embedding 층 바로 뒤에 Flatten 이나 Dense 클래스가 올 때 꼭 필요 

09-3 LSTM과 GRU 셀

LSTM(Long Short-Term Memory) / 단기 기억을 오래 기억하기 위해  고안된 순환층
-> 입력과 가중치를 곱하고 절편을 더해 활성화 함수를 통과시키는 구조를 여러개 가지고 있음 / 계산 결과는 타임스템에 재사용
-> 입력 게이트, 삭제 게이트, 출력 게이트 역할을 하는 작은 셀이 포함
-> 은닉 상태 외의 셀 상태를 출력 / 셀 상태는 다음 층으로 전달되지 않으며 현재 셀에서만 순환 
--> 첫 번째 매개변수에 뉴련의 개수 지정
--> dropout 매개변수에서 입력에 대한 드롭아웃 비율을 지정할 수 있음
--> return_sequences 매개변수에서 모든 타임스텝의 은닉 상태를 출력할지 결정 (기본값 False)

GRU / GRU 셀을 사용한 순환층 클래스 
-> 첫 번째 매개변수에 뉴련의 개수 지정
-> dropout 매개변수에서 입력에 대한 드롭아웃 비율을 지정할 수 있음
-> return_sequences 매개변수에서 모든 타임스텝의 은닉 상태를 출력할지 결정 (기본값 False)
